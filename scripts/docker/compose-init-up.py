#!/usr/bin/env python3
"""
Docker Compose Initialization and Startup Script

Main responsibilities:
- TOML-first configuration: prefer a TOML configuration (compose.config.sample.toml / compose.config.active.toml)
  and convert to a traditional `.env` when needed. This provides richer metadata
  (type, default, description, allowed_values) for variables.
- Migrate legacy `.env.sample` files if requested (opt-in via --env-legacy).
- Generate `.env` / `.env.active` from TOML or legacy samples with:
    - automatic password generation for *_PASSWORD descriptors,
    - token prompting for external tokens when interactive,
    - deferred-secret handling for values created by post-compose hooks.
- Validate Docker Compose configuration and environment variables
- Check image availability before starting containers
- Create required host directories with proper permissions
- Start Docker Compose services with configurable modes

#Filenames and precedence (how sample, project, local, global interact)
## compose.config.sample.toml (project sample / template)
Purpose: Checked into the project. Authoritative sample/tpl people edit. Contains metadata (project_name, env_tag, label_prefix), and recommended default variable metadata in [variables].
Use: When creating a new project, copy or adapt this sample to create a project compose.config.active.toml or commit it as the canonical sample.

## compose.config.active.toml (project active)
Purpose: Project-specific canonical runtime TOML file (what operators would generally modify for deployments).
Use: If present, it is considered the authoritative project configuration used by the runner to generate .env and to provide hook metadata and hook ordering. This file is a normal repo file but in some setups it may be maintained outside of source control.

## compose.config.global.toml (repo-level defaults)
Location: repo root (you requested the global config lie in the repo root).
Purpose: Repository-wide sensible defaults and shared metadata. Example uses: default LABEL_PREFIX for all projects in the repo, default control flags, default hooks to run for multiple projects.
Use: Read-only defaults merged in at runtime. Local project settings override these.


## Pre-Compose Hook Integration
- Hooks must be Python modules that define a class-based hook (PreComposeHook or PostComposeHook)
  with a run(env) method. This script imports hook modules and instantiates the
  expected hook class. Shell-based hooks (stdout/parsing) are not supported.
- Hooks may return generated secrets as a dict; these are persisted into the
  active environment as necessary.

Deferred secrets: Historically this project used the suffix `_DEFERED` (single
R) to mark variables that are not generated on first-run and are expected to be
created by a post-compose hook. This spelling is retained here for backwards
compatibility and appears in parsing/regexes (`*_PASSWORD_DEFERED`,
`*_TOKEN_DEFERED`). Do not change the suffix spelling in runtime code unless you
also update parsing logic and perform a repository-wide migration.

SINGLE SOURCE OF TRUTH
-----------------------
This file is the canonical, single source of truth for the compose-init helper
in this workspace: `/home/vb/repos/vbpub/scripts/docker/compose-init-up.py`.
Other copies of `compose-init-up.py` across this multi-repo workspace MUST be
replaced with a symlink to this file. The repository-level maintenance policy
is to update this file and then propagate changes by creating symlinks from
other locations.

If you are updating behavior, update this file only and run the repo-wide
symlink step to keep other copies in sync.

Usage:
    ./compose-init-up.py [options]
    
    Options:
        -d, --dir DIR     Working directory (default: current directory)
        -f, --file FILE   Docker compose file (default: docker-compose.yml)

Environment Configuration:
    By default the script prefers a TOML configuration (`compose.config.active.toml`)
    or a sample TOML file (`compose.config.sample.toml`). Legacy `.env.sample`
    processing is opt-in via `--env-legacy`. When running in TOML mode the
    script can generate a traditional `.env` file for use by Docker Compose.

Descriptor naming convention:
        You can encode generation hints into variable names so the script knows
        which charset and length to generate automatically. Pattern:

            NAME_{PASSWORD|TOKEN}_{ALNUM|HEX}{LENGTH}[_INTERNAL|_EXTERNAL|_DEFERED]

        Examples:
            - PORTUS_SECRET_KEY_BASE_TOKEN_ALNUM64  -> alphanumeric 64 chars
            - APP_API_TOKEN_HEX32                   -> hex 32 chars
            - SERVICE_ADMIN_PASSWORD_ALNUM32_DEFERED -> defer generation until post-compose

        Suffix meanings:
            - ALNUM:  A-Z, a-z, 0-9 (generated by default gen_pw())
            - HEX:    0-9, a-f (generated by secrets.token_hex)
            - LENGTH: decimal number describing requested length
            - INTERNAL: generate and store locally in .env
            - EXTERNAL: prompt user for secret (interactive)
            - DEFERED: leave blank now; expected to be created by post-compose hook


Error Handling:
    The script provides detailed error messages and stack traces for debugging.
    All library dependencies are checked before importing.
"""

import traceback
import sys
from typing import Dict, List, Tuple, Optional, Set, Any

def check_imports() -> None:
    """Check if all required standard library and third-party modules are available."""
    required_modules = [
        'os', 'sys', 're', 'subprocess', 'shutil', 'threading', 
        'concurrent.futures', 'queue', 'shlex', 'secrets', 'string'
    ]
    missing_modules = []
    for module in required_modules:
        try:
            __import__(module)
        except ImportError:
            missing_modules.append(module)
    if missing_modules:
        print(f"[ERROR] Missing required standard library modules: {', '.join(missing_modules)}", file=sys.stderr)
        print("[ERROR] This Python installation appears to be incomplete.", file=sys.stderr)
        sys.exit(1)

    # Check for non-standard library modules
    try:
        import yaml
    except ImportError:
        print("[ERROR] PyYAML library is required but not installed.", file=sys.stderr)
        print("[ERROR] Please install it with: pip install PyYAML", file=sys.stderr)
        sys.exit(1)
    try:
        import jinja2
    except ImportError:
        print("[ERROR] Jinja2 library is required but not installed.", file=sys.stderr)
        print("[ERROR] Please install it with: pip install Jinja2", file=sys.stderr)
        sys.exit(1)
# --- Jinja2 template rendering for dynamic YAML/config files ---
def render_templates(template_configs: Dict[str, Dict[str, str]], env_vars: Dict[str, str]) -> None:
    """
    Render templates using Jinja2 and write output files.
    Args:
        template_configs: Dict of template configs, e.g. { 'traefik_dynamic': { 'input': 'traefik_dynamic.yml.template', 'output': 'traefik_dynamic.yml' } }
        env_vars: Environment variables to use for rendering
    """
    try:
        import jinja2
    except ImportError:
        error("Jinja2 is required for template rendering but not installed.")
    env = jinja2.Environment(loader=jinja2.FileSystemLoader('.'), autoescape=False)
    for name, cfg in template_configs.items():
        input_path = cfg.get('input')
        output_path = cfg.get('output')
        if not input_path or not output_path:
            warn(f"Template config for {name} missing input/output paths; skipping.")
            continue
        try:
            template = env.get_template(input_path)
            output = template.render(**env_vars)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(output)
            info(f"Rendered template '{input_path}' to '{output_path}' using Jinja2.")
        except Exception as e:
            warn(f"Failed to render template {input_path} to {output_path}: {e}")

# Perform import check first
# Delay heavy import checks until runtime to allow unit tests to import the module
# without requiring optional runtime dependencies like PyYAML.
import os
import re
import subprocess
import shutil
import threading
import concurrent.futures
import queue
import shlex
import json
from pathlib import Path

import tomllib
try:
    from .config_schema import ProjectConfig, VariableConfig, parse_toml_config, config_to_env_dict, validate_config
except ImportError:
    # Fallback for direct script execution
    import sys
    sys.path.append(os.path.dirname(__file__))
    from config_schema import ProjectConfig, VariableConfig, parse_toml_config, config_to_env_dict, validate_config

def discover_global_toml_files(start_dir: str) -> list:
    """
    Walk up the directory tree from start_dir, collecting all compose.config.global.toml files.
    Returns a list of file paths, ordered furthest-away-first (repo root first).
    """
    files = []
    cur = os.path.abspath(start_dir)
    prev = None
    while cur != prev:
        candidate = os.path.join(cur, DEFAULT_GLOBAL_TOML_FILE)
        if os.path.isfile(candidate):
            files.insert(0, candidate)  # insert at front for furthest-away-first
        prev = cur
        cur = os.path.dirname(cur)
    return files

def merge_global_toml_files(global_files: list) -> dict:
    """
    Merge all discovered global TOML files into a single dict.
    Furthest-away files are merged first, so closer files override.
    """
    merged = {}
    for f in global_files:
        try:
            with open(f, 'rb') as fin:
                data = tomllib.load(fin)
            merged.update(data)
        except Exception as e:
            warn(f"Failed to load global TOML file {f}: {e}")
    return merged

def collect_global_vars(merged_toml: dict) -> dict:
    """
    Collect variables from a [global-vars] section, if present.
    Returns a dict of global variables.
    """
    return merged_toml.get('global-vars', {})
import urllib.request
import socket

# --- Output helpers ---
COLOR_RED = '\033[0;31m'
COLOR_GREEN = '\033[0;32m'
COLOR_YELLOW = '\033[1;33m'
COLOR_BLUE = '\033[0;34m'
COLOR_UNSET = '\033[0m'

def step(msg: str) -> None:
    """Print a major step or milestone message."""
    print(f"{COLOR_BLUE}==>{COLOR_UNSET} {msg}")

def info(msg: str) -> None:
    """Print an informational message."""
    print(f"{COLOR_GREEN}[INFO]{COLOR_UNSET} {msg}")

def warn(msg: str) -> None:
    """Print a warning message."""
    print(f"{COLOR_YELLOW}[WARN]{COLOR_UNSET} {msg}")

def error(msg: str, show_stacktrace: bool = True) -> None:
    """Print an error message with optional stack trace and exit."""
    print(f"{COLOR_RED}[ERROR]{COLOR_UNSET} {msg}", file=sys.stderr)
    
    if show_stacktrace:
        print(f"{COLOR_RED}[ERROR]{COLOR_UNSET} Stack trace:", file=sys.stderr)
        # Get the current stack, excluding this error function
        stack = traceback.extract_stack()[:-1]
        for frame in stack:
            print(f"{COLOR_RED}[ERROR]{COLOR_UNSET}   at {frame.filename}:{frame.lineno} in {frame.name}()", file=sys.stderr)
            if frame.line:
                print(f"{COLOR_RED}[ERROR]{COLOR_UNSET}     {frame.line.strip()}", file=sys.stderr)
    
    sys.exit(1)



# --- Canonical file names and config variable definitions (maintainers: edit here) ---
# All file names used by this script, for easy maintenance
DEFAULT_ENV_ACTIVE_FILE = ".env"
DEFAULT_ENV_SAMPLE_FILE = ".env.sample"
DEFAULT_TOML_ACTIVE_FILE = "compose.config.active.toml"
DEFAULT_TOML_SAMPLE_FILE = "compose.config.sample.toml"
DEFAULT_GLOBAL_TOML_FILE = "compose.config.global.toml"
DEFAULT_LOCAL_TOML_FILE = "compose.config.local.toml"  # legacy name; no longer used for persistence

# Endpoints to query for public IP detection. The generator will try these in order
# and stop on the first successful response. You can override by setting the
# environment variable COMPINIT_IP_DETECT_URLS as a comma-separated list.
DEFAULT_IP_DETECT_URLS = [
    "https://api.ipify.org/",        # simple plaintext IP
    "https://ifconfig.me/ip",       # plaintext IP
    "https://ident.me/"             # plaintext IP
]

# Canonical config variable definitions: name, default, description, type
CONFIG_VARIABLES = [
    {
        "name": "COMPINIT_COMPOSE_START_MODE",
        "default": "abort-on-failure",
        "description": "Control flow for compose-init-up",
        "allowed_values": ["abort-on-failure", "continue-on-error", "best-effort"],
        "type": "string"
    },
    {
        "name": "COMPINIT_RESET_BEFORE_START",
        "default": "none",
        "description": "Reset actions before starting",
        "allowed_values": ["none", "containers", "named-volumes", "hostdirs", "all"],
        "type": "string"
    },
    {
        "name": "COMPINIT_CHECK_IMAGE_ENABLED",
        "default": "false",
        "description": "Enable/disable image availability checks before compose up",
        "type": "bool"
    },
    {
        "name": "COMPINIT_IMAGE_CHECK_CONTINUE_ON_ERROR",
        "default": "0",
        "description": "Continue on image check error (0/1)",
        "type": "int"
    },
    {
        "name": "COMPINIT_DEPENDENCIES",
        "default": "",
        "description": "Comma-separated list of dependency directories to start first",
        "type": "string"
    },
    {
        "name": "COMPINIT_HOOK_PRE_COMPOSE",
        "default": "",
        "description": "Python script to run before compose up",
        "type": "string"
    },
    {
        "name": "COMPINIT_HOOK_POST_COMPOSE",
        "default": "",
        "description": "Python script to run after compose up",
        "type": "string"
    },
    {
        "name": "COMPINIT_MYREGISTRY_URL",
        "default": "",
        "description": "Optional registry mirror URL",
        "type": "string"
    },
    {
        "name": "COMPINIT_STRICT_MODE",
        "default": "",
        "description": "Global strict mode toggle (1 to enable)",
        "type": "int"
    },
    {
        "name": "COMPINIT_ENABLE_ENV_EXPANSION",
        "default": "false",
        "description": "Enable $VAR and $(cmd) expansion in env values at runtime",
        "type": "bool"
    },
    {
        "name": "PUBLIC_FQDN",
        "default": "example.local",
        "description": "Public DNS name for service",
        "type": "string"
    },
    {
        "name": "PUBLIC_TLS_KEY_PEM",
        "default": "/etc/letsencrypt/live/$PUBLIC_FQDN/privkey.pem",
        "description": "Path to TLS private key PEM",
        "type": "string"
    },
    {
        "name": "PUBLIC_TLS_CRT_PEM",
        "default": "/etc/letsencrypt/live/$PUBLIC_FQDN/fullchain.pem",
        "description": "Path to TLS certificate PEM",
        "type": "string"
    },
    {
        "name": "HEALTHCHECK_INTERVAL",
        "default": "10s",
        "description": "Healthcheck interval",
        "type": "string"
    },
    {
        "name": "HEALTHCHECK_TIMEOUT",
        "default": "5s",
        "description": "Healthcheck timeout",
        "type": "string"
    },
    {
        "name": "HEALTHCHECK_RETRIES",
        "default": "3",
        "description": "Healthcheck retries",
        "type": "int"
    },
    {
        "name": "HEALTHCHECK_START_PERIOD",
        "default": "30s",
        "description": "Healthcheck start period",
        "type": "string"
    },
    {
        "name": "LABEL_PROJECT_NAME",
        "default": "my-project",
        "description": "Project name label",
        "type": "string"
    },
    {
        "name": "LABEL_ENV_TAG",
        "default": "dev",
        "description": "Environment tag (dev/staging/prod)",
        "type": "string"
    },
    {
        "name": "LABEL_PREFIX",
        "default": "example.com",
        "description": "Label prefix for project",
        "type": "string"
    },
    {
        "name": "UID",
        "default": "$(id -u)",
        "description": "User ID for container",
        "type": "string"
    },
    {
        "name": "GID",
        "default": "$(id -g)",
        "description": "Group ID for container",
        "type": "string"
    },
    # Add more variables as needed
]


def handle_exception(exc_type: type, exc_value: Exception, exc_traceback: Any) -> None:
    """Global exception handler that provides detailed error information."""
    if issubclass(exc_type, KeyboardInterrupt):
        print(f"\n{COLOR_YELLOW}[WARN]{COLOR_UNSET} Operation interrupted by user", file=sys.stderr)
        sys.exit(1)
    
    print(f"{COLOR_RED}[ERROR]{COLOR_UNSET} Unhandled exception occurred:", file=sys.stderr)
    print(f"{COLOR_RED}[ERROR]{COLOR_UNSET} {exc_type.__name__}: {exc_value}", file=sys.stderr)
    print(f"{COLOR_RED}[ERROR]{COLOR_UNSET} Stack trace:", file=sys.stderr)
    
    tb_lines = traceback.format_exception(exc_type, exc_value, exc_traceback)
    for line in tb_lines:
        for subline in line.rstrip().split('\n'):
            if subline.strip():
                print(f"{COLOR_RED}[ERROR]{COLOR_UNSET}   {subline}", file=sys.stderr)
    
    sys.exit(1)

# Set up global exception handler
sys.excepthook = handle_exception

# --- Utility functions ---
def gen_pw(length: int = 16, charset: str = 'ALNUM') -> str:
    """
    Generate a secure random password.
    
    Args:
        length: Length of the password to generate (default: 16)
        
    Returns:
        A secure random password containing letters and digits
        
    Raises:
        ValueError: If length is less than 1
    """
    if length < 1:
        raise ValueError("Password length must be at least 1")

    try:
        import secrets
        import string
        cs = (charset or 'ALNUM').upper()
        if cs == 'ALNUM':
            alphabet = string.ascii_letters + string.digits
            return ''.join(secrets.choice(alphabet) for _ in range(length))
        elif cs == 'HEX':
            # token_hex returns 2*nbytes hex chars, so compute nbytes and trim
            nbytes = (length + 1) // 2
            return secrets.token_hex(nbytes)[:length]
        else:
            error(f"Unsupported charset for gen_pw: {charset}")
    except ImportError:
        error("Required modules 'secrets' or 'string' not available for password generation")


def strict_flag(env: Dict[str, str], name: str) -> bool:
    """
    Check whether a strict flag is enabled using the canonical
    COMPINIT_STRICT_<NAME> environment variable. Legacy
    COMPINIT_<NAME>_STRICT variables are no longer supported.

    Args:
        env: environment mapping to check
        name: uppercase flag name, e.g. 'EXTERNAL' or 'EXPANSION'

    Returns:
        True if the strict flag is enabled, False otherwise
    """
    key = f'COMPINIT_STRICT_{name}'
    val = env.get(key, None)
    if val is None:
        return False
    return str(val).lower() in ('1', 'true', 'yes')

def expand_vars(val: str, env: Dict[str, str]) -> str:
    """
    Expand $(...) command substitution and $VAR environment variables in a string.
    
    Args:
        val: The string containing variables to expand
        env: Environment dictionary for variable substitution
        
    Returns:
        The string with variables expanded
        
    Note:
        Command substitution $(...) is executed in a shell subprocess.
        Environment variables $VAR are replaced from the env dictionary.
    """
    if not isinstance(val, str):
        return str(val)
    
    # Expand $(...) command substitutions
    def repl(m: re.Match[str]) -> str:
        cmd = m.group(1)
        try:
            result = subprocess.check_output(cmd, shell=True, env=env, stderr=subprocess.DEVNULL)
            return result.decode().strip()
        except subprocess.CalledProcessError as e:
            # In strict expansion mode, treat command substitution failures as fatal
            if strict_flag(env, 'EXPANSION'):
                error(f"Command substitution failed for '{cmd}': {e}")
            warn(f"Command substitution failed for '{cmd}': {e}")
            return ""
        except Exception as e:
            if strict_flag(env, 'EXPANSION'):
                error(f"Error in command substitution '{cmd}': {e}")
            warn(f"Error in command substitution '{cmd}': {e}")
            return ""
    
    val = re.sub(r'\$\(([^)]+)\)', repl, val)
    
    # Expand $VAR environment variables
    # Support both ${VAR} and $VAR styles
    # Replace ${VAR} style first, then $VAR. If a variable is missing and
    # COMPINIT_EXPANSION_STRICT is set, raise a fatal error; otherwise warn
    # and substitute an empty string.
    def var_repl_braced(m: re.Match[str]) -> str:
        name = m.group(1)
        if name in env:
            return env[name]
        if strict_flag(env, 'EXPANSION'):
            error(f"Undefined variable '{{{name}}}' during expansion (COMPINIT_EXPANSION_STRICT=1)")
        warn(f"Undefined variable '{{{name}}}' during expansion; substituting empty string")
        return ""

    def var_repl_plain(m: re.Match[str]) -> str:
        name = m.group(1)
        if name in env:
            return env[name]
        if strict_flag(env, 'EXPANSION'):
            error(f"Undefined variable '${name}' during expansion (COMPINIT_EXPANSION_STRICT=1)")
        warn(f"Undefined variable '${name}' during expansion; substituting empty string")
        return ""

    val = re.sub(r'\$\{([A-Za-z_][A-Za-z0-9_]*)\}', var_repl_braced, val)
    val = re.sub(r'\$([A-Za-z_][A-Za-z0-9_]*)', var_repl_plain, val)
    
    return val


def expand_cmds_only(val: str, env: Dict[str, str]) -> str:
    """
    Expand only command substitutions $(...) in the string using the provided env.
    Do NOT perform $VAR environment variable replacement here; that preserves
    literal $VAR tokens in generated env files.
    """
    if not isinstance(val, str):
        return str(val)

    def repl(m: re.Match[str]) -> str:
        cmd = m.group(1)
        try:
            result = subprocess.check_output(cmd, shell=True, env=env, stderr=subprocess.DEVNULL)
            return result.decode().strip()
        except subprocess.CalledProcessError as e:
            if strict_flag(env, 'EXPANSION'):
                error(f"Command substitution failed for '{cmd}': {e}")
            warn(f"Command substitution failed for '{cmd}': {e}")
            return ""
        except Exception as e:
            if strict_flag(env, 'EXPANSION'):
                error(f"Error in command substitution '{cmd}': {e}")
            warn(f"Error in command substitution '{cmd}': {e}")
            return ""

    return re.sub(r'\$\(([^)]+)\)', repl, val)

def parse_env_sample(sample_path: str, env_path: str) -> None:
    """
    Parse .env.sample file and generate .env.active with auto-generated passwords.
    
    This function reads a sample environment file and creates an active environment
    file by:
    - Auto-generating secure passwords for variables ending with '_PASSWORD'
    - Prompting for user input on variables ending with '_TOKEN' that are empty
    - Copying all other variables as-is
    
    Args:
        sample_path: Path to the .env.sample file
        env_path: Path to the .env.active file to create
        
    Raises:
        FileNotFoundError: If sample_path doesn't exist
        PermissionError: If unable to write to env_path
        IOError: If file operations fail
    """
    if not os.path.isfile(sample_path):
        error(f"Sample environment file '{sample_path}' not found")
    
    # Check if we can write to the target directory
    target_dir = os.path.dirname(env_path) or '.'
    if not os.access(target_dir, os.W_OK):
        error(f"Cannot write to directory '{target_dir}' for file '{env_path}'")
    
    env = dict(os.environ)
    # By default do not expand $VAR or $(...) in sample values unless explicitly enabled
    expansion_enabled = os.environ.get('COMPINIT_ENABLE_ENV_EXPANSION', '').lower() in ('1', 'true', 'yes')

    def _maybe_expand(val: str, local_env: Dict[str, str]) -> str:
        """
        Expand command substitutions $(...) always (so generated files don't
        contain literal $(...) or backticks). Only expand $VAR / ${VAR}
        when COMPINIT_ENABLE_ENV_EXPANSION is enabled. This preserves
        literal $VAR tokens in generated .env files by default (test
        expectation).
        """
        try:
            # First, expand command substitutions only to produce literal
            # outputs for things like UID=$(id -u) while preserving $VAR tokens.
            res = expand_cmds_only(val, local_env)
        except Exception:
            res = val

        if expansion_enabled:
            # Expand only variable references (${VAR} and $VAR) using local_env
            # without re-running command substitutions.
            def var_repl_braced(m: re.Match[str]) -> str:
                name = m.group(1)
                return local_env.get(name, '')

            def var_repl_plain(m: re.Match[str]) -> str:
                name = m.group(1)
                return local_env.get(name, '')

            try:
                res = re.sub(r'\$\{([A-Za-z_][A-Za-z0-9_]*)\}', var_repl_braced, res)
                res = re.sub(r'\$([A-Za-z_][A-Za-z0-9_]*)', var_repl_plain, res)
            except Exception:
                pass

        return res
    
    try:
        with open(sample_path, 'r', encoding='utf-8') as fin, \
             open(env_path, 'w', encoding='utf-8') as fout:
            
            local_env = dict(os.environ)
            for line_num, line in enumerate(fin, 1):
                orig = line.rstrip('\n')
                
                # Copy empty lines and comments as-is
                if not orig.strip() or orig.strip().startswith('#'):
                    fout.write(orig + '\n')
                    continue
                
                # Match password variables: VAR_PASSWORD=value # comment
                m_pw = re.match(r'^([A-Za-z0-9_]+_PASSWORD)=(.*?)([ \t#].*)?$', orig)
                # Accept both historical misspelling 'DEFERED' and the common 'DEFERRED'
                m_pw_defer = re.match(r'^([A-Za-z0-9_]+_PASSWORD_(?:DEFERED|DEFERRED))=(.*?)([ \t#].*)?$', orig)
                m_token_internal = re.match(r'^([A-Za-z0-9_]+_TOKEN_INTERNAL)=(.*?)([ \t#].*)?$', orig)
                m_token_external = re.match(r'^([A-Za-z0-9_]+_TOKEN_EXTERNAL)=(.*?)([ \t#].*)?$', orig)
                m_token_defer = re.match(r'^([A-Za-z0-9_]+_TOKEN_(?:DEFERED|DEFERRED))=(.*?)([ \t#].*)?$', orig)
                # New descriptor patterns: NAME_TOKEN_ALNUM64 or NAME_TOKEN_HEX32, optional trailing
                # suffix _INTERNAL/_EXTERNAL/_DEFERED or _DEFERRED (accept both spellings)
                m_token_desc = re.match(r'^([A-Za-z0-9_]+_TOKEN_(ALNUM|HEX)(\d+))(?:_(INTERNAL|EXTERNAL|DEFERED|DEFERRED))?=(.*?)([ \t#].*)?$', orig)
                m_pw_desc = re.match(r'^([A-Za-z0-9_]+_PASSWORD_(ALNUM|HEX)(\d+))(?:_(?:DEFERED|DEFERRED))?=(.*?)([ \t#].*)?$', orig)
                # SECRET_KEY_BASE is not treated specially by descriptor parsing anymore.
                # Use NAME_PASSWORD_*/NAME_TOKEN_* descriptor forms to request generation.
                # Legacy VAR_TOKEN is no longer supported; use _TOKEN_EXTERNAL or _TOKEN_INTERNAL or the descriptor forms
                
                try:
                    if m_pw:
                        key, val, comment = m_pw.group(1), m_pw.group(2).strip(), m_pw.group(3) or ''
                        if not val:
                            info(f"Generating random password for {key}")
                            val = gen_pw(20)
                        elif 'set password manually later' in val.lower():
                            warn(f"{key} contains a placeholder. Please set a secure password!")
                        # Expand any command substitutions or variable refs so the generated
                        # live env file contains literal values (e.g., UID=$(id -u) -> 1000)
                        write_val = _maybe_expand(val, local_env)
                        fout.write(f"{key}={write_val}{comment}\n")
                        local_env[key] = write_val
                    # SECRET_KEY_BASE descriptor handling removed: use NAME_PASSWORD_* or NAME_TOKEN_* descriptors instead
                    elif m_token_desc:
                        # Descriptor token, e.g. NAME_TOKEN_ALNUM64 or NAME_TOKEN_HEX32, optional trailing INTERNAL/EXTERNAL/DEFERED
                        base_key = m_token_desc.group(1)  # e.g. NAME_TOKEN_ALNUM64
                        charset = m_token_desc.group(2)
                        length = int(m_token_desc.group(3))
                        trailing = m_token_desc.group(4)  # INTERNAL|EXTERNAL|DEFERED or None
                        val = m_token_desc.group(5).strip()
                        comment = m_token_desc.group(6) or ''
                        # construct the actual key name including trailing suffix if present
                        actual_key = base_key + (f"_{trailing}" if trailing else '')
                        if trailing == 'DEFERED':
                            fout.write(f"{actual_key}={val}{comment}\n")
                            local_env[actual_key] = ''
                        else:
                            if not val:
                                info(f"Generating token for {actual_key} ({charset}, len={length})")
                                if charset == 'ALNUM':
                                    gen_val = gen_pw(length)
                                else:
                                    import secrets as _secrets
                                    gen_val = _secrets.token_hex((length + 1) // 2)[:length]
                                val = gen_val
                            try:
                                write_val = _maybe_expand(val, local_env)
                            except Exception:
                                write_val = val
                            fout.write(f"{actual_key}={write_val}{comment}\n")
                            local_env[actual_key] = write_val
                            # Do NOT write the base descriptor name when a trailing suffix
                            # (INTERNAL/EXTERNAL/DEFERED) is present. Compose files should
                            # reference the fully-suffixed variable name (e.g. VAR_TOKEN_HEX32_INTERNAL).

                    elif m_pw_desc:
                        # Descriptor password e.g. NAME_PASSWORD_ALNUM64 (DEFERED allowed)
                        full_key = m_pw_desc.group(1)
                        charset = m_pw_desc.group(2)
                        length = int(m_pw_desc.group(3))
                        val = m_pw_desc.group(4).strip()
                        comment = m_pw_desc.group(5) or ''
                        if val == '' and not full_key.endswith('_DEFERED'):
                            info(f"Generating password for {full_key} ({charset}, len={length})")
                            if charset == 'ALNUM':
                                gen_val = gen_pw(length)
                            else:
                                import secrets as _secrets
                                gen_val = _secrets.token_hex((length + 1) // 2)[:length]
                            val = gen_val
                        try:
                            write_val = _maybe_expand(val, local_env)
                        except Exception:
                            write_val = val
                        fout.write(f"{full_key}={write_val}{comment}\n")
                        local_env[full_key] = write_val

                    elif m_token_internal:
                        # Plain VAR_TOKEN_INTERNAL -> generate a default internal token if empty
                        key, val, comment = m_token_internal.group(1), m_token_internal.group(2).strip(), m_token_internal.group(3) or ''
                        if not val:
                            info(f"Generating internal token for {key}")
                            val = gen_pw(40)
                        try:
                            write_val = _maybe_expand(val, local_env)
                        except Exception:
                            write_val = val
                        fout.write(f"{key}={write_val}{comment}\n")
                        local_env[key] = write_val
                    elif m_token_defer:
                        key, val, comment = m_token_defer.group(1), m_token_defer.group(2).strip(), m_token_defer.group(3) or ''
                        fout.write(f"{key}={val}{comment}\n")
                        local_env[key] = ''
                    elif m_token_external:
                        key, val, comment = m_token_external.group(1), m_token_external.group(2).strip(), m_token_external.group(3) or ''
                        if not val:
                            # First prefer a value supplied in the process environment
                            env_fallback = os.environ.get(key)
                            if env_fallback:
                                info(f"Using provided environment value for external token {key}")
                                val = env_fallback
                            else:
                                # If configured to be strict, abort with an error in non-interactive mode
                                strict = strict_flag(os.environ, 'EXTERNAL')
                                if os.environ.get('COMPINIT_ASSUME_YES', '').lower() in ('1', 'true', 'yes') and strict:
                                    error(f"External token {key} requires input and COMPINIT_STRICT_EXTERNAL=1 enforces strict behavior in non-interactive runs")
                                # If running non-interactive and not strict, warn and leave empty
                                if os.environ.get('COMPINIT_ASSUME_YES', '').lower() in ('1', 'true', 'yes'):
                                    warn(f"{key} is marked EXTERNAL but no value provided; continuing in non-interactive mode with empty value")
                                    val = ''
                                else:
                                    warn(f"{key} is marked EXTERNAL and requires a secret. Please enter the value:")
                                    try:
                                        val = input(f"{key}=").strip()
                                    except (EOFError, KeyboardInterrupt):
                                        error(f"User input required for {key} but not provided")
                        try:
                            write_val = expand_vars(val, local_env)
                        except Exception:
                            write_val = val
                        fout.write(f"{key}={write_val}{comment}\n")
                        local_env[key] = write_val

                    # NOTE: SECRET_KEY_BASE-specific descriptor support has been removed.
                    # Use NAME_PASSWORD_* or NAME_TOKEN_* descriptor forms to request generation.

                    # Legacy VAR_TOKEN is no longer supported; please use VAR_TOKEN_INTERNAL or VAR_TOKEN_EXTERNAL
                        
                    else:
                        if '=' in orig:
                            key, val = orig.split('=', 1)
                            key = key.strip()
                            val = val.strip()
                            # Detect surrounding quotes and preserve them in the generated file.
                            quote_char = None
                            inner = val
                            if (val.startswith('"') and val.endswith('"')) or (val.startswith("'") and val.endswith("'")):
                                quote_char = val[0]
                                inner = val[1:-1]

                            # Expand any $(...) or $VAR references now so the generated env
                            # contains literal values instead of runtime substitutions.
                            try:
                                write_val = _maybe_expand(inner, local_env)
                            except Exception:
                                write_val = inner

                            # If the original value was quoted, re-wrap it with the same quote
                            # and escape any inner occurrence of that quote character.
                            if quote_char:
                                esc = write_val.replace(quote_char, '\\' + quote_char)
                                fout.write(f"{key}={quote_char}{esc}{quote_char}\n")
                                local_env[key] = esc
                            else:
                                fout.write(f"{key}={write_val}\n")
                                local_env[key] = write_val
                        else:
                            fout.write(orig + '\n')
                        
                except Exception as e:
                    error(f"Error processing line {line_num} in {sample_path}: {e}")
                    
    except FileNotFoundError:
        error(f"Sample file '{sample_path}' not found")
    except PermissionError:
        error(f"Permission denied writing to '{env_path}'")
    except IOError as e:
        error(f"I/O error processing environment files: {e}")
    except Exception as e:
        error(f"Unexpected error parsing environment file: {e}")

def parse_toml_sample(toml_path: str, env_path: str) -> None:
    """
    Parse .env.toml file and generate .env with auto-generated passwords and tokens.
    
    This function reads a TOML configuration file and creates a traditional .env
    file by:
    - Auto-generating secure passwords for variables with type="password"
    - Prompting for user input on variables with source="external" that are empty
    - Processing hooks and dependencies
    - Copying all other variables as-is
    
    Args:
        toml_path: Path to the .env.toml file
        env_path: Path to the .env file to create
        
    Raises:
        FileNotFoundError: If toml_path doesn't exist
        ValueError: If TOML configuration is invalid
        IOError: If file operations fail
    """
    if not os.path.isfile(toml_path):
        error(f"TOML configuration file '{toml_path}' not found")
    
    # Check if we can write to the target directory
    target_dir = os.path.dirname(env_path) or '.'
    if not os.access(target_dir, os.W_OK):
        error(f"Cannot write to directory '{target_dir}' for file '{env_path}'")
    
    try:
        # Parse TOML configuration
        config = parse_toml_config(toml_path)
        
        # Validate configuration
        validation_errors = validate_config(config)
        if validation_errors:
            error(f"TOML configuration validation failed:\n  - " + "\n  - ".join(validation_errors))
        
        info(f"Parsed TOML configuration for project: {config.project_name}")
        
        # Generate values for variables that need generation
        generated_values = {}
        local_env = dict(os.environ)
        
        for name, var_config in config.variables.items():
            if var_config.source == "deferred":
                generated_values[name] = ""  # Will be set by hooks
                continue
            
            if var_config.type == "password" and not var_config.value:
                info(f"Generating password for {name} (type={var_config.type}, length={var_config.length})")
                generated_values[name] = gen_pw(var_config.length or 32, var_config.charset)
            elif var_config.type == "token" and not var_config.value:
                if var_config.source == "external":
                    # Handle external tokens
                    env_fallback = os.environ.get(name)
                    if env_fallback:
                        info(f"Using provided environment value for external token {name}")
                        generated_values[name] = env_fallback
                    else:
                        # Prompt for external token or use strict mode handling
                        strict = strict_flag(os.environ, 'EXTERNAL')
                        if os.environ.get('COMPINIT_ASSUME_YES', '').lower() in ('1', 'true', 'yes') and strict:
                            error(f"External token {name} requires input and COMPINIT_STRICT_EXTERNAL=1 enforces strict behavior")
                        if os.environ.get('COMPINIT_ASSUME_YES', '').lower() in ('1', 'true', 'yes'):
                            warn(f"{name} is marked external but no value provided; continuing with empty value")
                            generated_values[name] = ''
                        else:
                            desc = var_config.description or f"External {var_config.type}"
                            warn(f"{name} ({desc}) requires a value. Please enter:")
                            try:
                                generated_values[name] = input(f"{name}=").strip()
                            except (EOFError, KeyboardInterrupt):
                                error(f"User input required for {name} but not provided")
                elif var_config.source == "internal":
                    info(f"Generating internal token for {name} (length={var_config.length})")
                    generated_values[name] = gen_pw(var_config.length or 64, var_config.charset)
                else:
                    generated_values[name] = var_config.value or ""
            elif var_config.value is not None:
                # Use provided value, potentially with expansion
                try:
                    expanded = expand_vars(var_config.value, local_env)
                    generated_values[name] = expanded
                    local_env[name] = expanded
                except Exception:
                    generated_values[name] = var_config.value
                    local_env[name] = var_config.value
            else:
                generated_values[name] = ""
        
        # Convert config to env dict
        env_dict = config_to_env_dict(config, generated_values)
        
        # Write .env file
        with open(env_path, 'w', encoding='utf-8') as f:
            f.write(f"# Generated from {toml_path}\n")
            f.write(f"# Project: {config.project_name} ({config.env_tag})\n\n")
            
            # Write variables in logical groups
            f.write("# Control flow\n")
            for key in ['COMPINIT_COMPOSE_START_MODE', 'COMPINIT_RESET_BEFORE_START', 
                       'COMPINIT_CHECK_IMAGE_ENABLED', 'COMPINIT_IMAGE_CHECK_CONTINUE_ON_ERROR']:
                if key in env_dict:
                    f.write(f"{key}={env_dict[key]}\n")
            
            f.write("\n# Project metadata\n")
            for key in ['LABEL_PROJECT_NAME', 'LABEL_ENV_TAG', 'LABEL_PREFIX']:
                if key in env_dict:
                    f.write(f"{key}={env_dict[key]}\n")
            
            f.write("\n# Infrastructure\n")
            for key in ['PUBLIC_FQDN', 'PUBLIC_TLS_KEY_PEM', 'PUBLIC_TLS_CRT_PEM']:
                if key in env_dict:
                    f.write(f"{key}={env_dict[key]}\n")
            
            f.write("\n# Dependencies and hooks\n")
            for key in ['COMPINIT_DEPENDENCIES', 'COMPINIT_HOOK_PRE_COMPOSE', 'COMPINIT_HOOK_POST_COMPOSE']:
                if key in env_dict:
                    f.write(f"{key}={env_dict[key]}\n")
            
            f.write("\n# Health monitoring\n")
            for key in ['HEALTHCHECK_INTERVAL', 'HEALTHCHECK_TIMEOUT', 'HEALTHCHECK_RETRIES', 'HEALTHCHECK_START_PERIOD']:
                if key in env_dict:
                    f.write(f"{key}={env_dict[key]}\n")
            
            f.write("\n# User variables\n")
            for key in ['UID', 'GID']:
                if key in env_dict:
                    f.write(f"{key}={env_dict[key]}\n")
            
            f.write("\n# Project variables\n")
            for name, var_config in config.variables.items():
                value = generated_values.get(name, var_config.value or "")
                if var_config.description:
                    f.write(f"# {var_config.description}\n")
                
                # Preserve quotes for string values that need them
                if var_config.type == "string" and value and (' ' in value or "'" in value):
                    escaped = value.replace('"', '\\"')
                    f.write(f'{name}="{escaped}"\n')
                else:
                    f.write(f"{name}={value}\n")
            
            f.write("\n# Additional variables\n")
            for key, value in env_dict.items():
                if key not in config.variables and not any(key.startswith(prefix) for prefix in 
                    ['COMPINIT_', 'LABEL_', 'PUBLIC_', 'HEALTHCHECK_', 'UID', 'GID']):
                    f.write(f"{key}={value}\n")
        
        info(f"Generated .env file from TOML configuration: {env_path}")
        
    except Exception as e:
        error(f"Failed to process TOML configuration: {e}")


def load_env_file(env_file: str) -> Tuple[Dict[str, str], List[str]]:
    """
    Load environment variables from a .env file.
    
    Args:
        env_file: Path to the environment file
        
    Returns:
        Tuple of (env_vars dict, loaded_keys list)
        
    Raises:
        FileNotFoundError: If env_file doesn't exist
        IOError: If file cannot be read
    """
    if not os.path.isfile(env_file):
        error(f"Environment file '{env_file}' not found")
    
    env_vars = {}
    loaded_keys = []
    
    try:
        with open(env_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                
                # Skip empty lines and comments
                if not line or line.startswith('#'):
                    continue
                
                # Remove inline comments (but preserve # in values)
                if '#' in line:
                    # Find the first # that's not inside quotes
                    in_quotes = False
                    quote_char = None
                    for i, char in enumerate(line):
                        if char in ('"', "'") and (i == 0 or line[i-1] != '\\'):
                            if not in_quotes:
                                in_quotes = True
                                quote_char = char
                            elif char == quote_char:
                                in_quotes = False
                                quote_char = None
                        elif char == '#' and not in_quotes:
                            line = line[:i].strip()
                            break
                
                if '=' not in line:
                    warn(f"Skipping invalid line {line_num} in {env_file}: '{line}'")
                    continue
                
                key, val = line.split('=', 1)
                key = key.strip()
                val = val.strip()
                
                # Remove quotes from values if present
                if val.startswith('"') and val.endswith('"'):
                    val = val[1:-1]
                elif val.startswith("'") and val.endswith("'"):
                    val = val[1:-1]
                
                env_vars[key] = val
                loaded_keys.append(key)
                
    except FileNotFoundError:
        error(f"Environment file '{env_file}' not found")
    except PermissionError:
        error(f"Permission denied reading '{env_file}'")  
    except IOError as e:
        error(f"I/O error reading environment file '{env_file}': {e}")
    except Exception as e:
        error(f"Unexpected error loading environment file '{env_file}': {e}")
    
    return env_vars, loaded_keys


def persist_env_vars(env_file: str, new_vars: Dict[str, str]) -> None:
    """
    Persist or update key=value pairs into the env_file. If a key exists, replace its value.
    Otherwise, append the new key=value at the end.
    """
    try:
        # Read existing lines
        lines = []
        if os.path.isfile(env_file):
            with open(env_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()

        # Build map of existing keys to line index
        key_line_map = {}
        for idx, raw in enumerate(lines):
            s = raw.strip()
            if not s or s.startswith('#') or '=' not in s:
                continue
            k = s.split('=', 1)[0].strip()
            key_line_map[k] = idx

        # Update existing keys or append
        for k, v in new_vars.items():
            entry = f"{k}={v}\n"
            if k in key_line_map:
                lines[key_line_map[k]] = entry
            else:
                lines.append(entry)

        # Write back atomically
        tmp = env_file + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as f:
            f.writelines(lines)
        os.replace(tmp, env_file)
        info(f"Persisted {len(new_vars)} variables into {env_file}")
    except Exception as e:
        warn(f"Failed to persist variables into {env_file}: {e}")


def resolve_dependencies(env_vars: Dict[str, str], current_dir: str, visited: Optional[Set[str]] = None) -> List[str]:
    """
    Resolve dependency chain from COMPINIT_DEPENDENCIES variable.
    Returns list of absolute dependency paths in start order (dependencies first).
    
    Args:
        env_vars: Environment variables containing COMPINIT_DEPENDENCIES
        current_dir: Current project directory (absolute path)
        visited: Set of visited directories for cycle detection
        
    Returns:
        List of absolute paths to start in order
        
    Raises:
        ValueError: If dependency cycle detected or invalid path
    """
    if visited is None:
        visited = set()
    
    current_abs = os.path.abspath(current_dir)
    if current_abs in visited:
        error(f"Dependency cycle detected: {current_abs} is already in the dependency chain")
    
    visited.add(current_abs)
    dependencies = []
    
    # Parse COMPINIT_DEPENDENCIES (comma or colon separated relative/absolute paths)
    deps_str = env_vars.get('COMPINIT_DEPENDENCIES', '').strip()
    if not deps_str:
        return [current_abs]  # No dependencies, just return self
    
    dep_paths = [d.strip() for d in deps_str.replace(':', ',').split(',') if d.strip()]
    
    for dep_path in dep_paths:
        # Convert relative paths to absolute (relative to current project)
        if not os.path.isabs(dep_path):
            dep_abs = os.path.abspath(os.path.join(current_dir, dep_path))
        else:
            dep_abs = os.path.abspath(dep_path)
        
        if not os.path.isdir(dep_abs):
            error(f"Dependency directory does not exist: {dep_abs}")
        
        # Check for .env.sample in dependency
        dep_sample = os.path.join(dep_abs, '.env.sample')
        if not os.path.isfile(dep_sample):
            warn(f"Dependency {dep_abs} has no .env.sample, skipping")
            continue
        
        # Recursively resolve dependencies of this dependency
        try:
            # Load dependency's env to check for its own dependencies
            dep_env_vars, _ = load_env_file(os.path.join(dep_abs, '.env.sample'))
            sub_deps = resolve_dependencies(dep_env_vars, dep_abs, visited.copy())
            dependencies.extend(sub_deps)
        except Exception as e:
            warn(f"Failed to resolve dependencies for {dep_abs}: {e}")
            dependencies.append(dep_abs)
    
    # Remove duplicates while preserving order
    seen = set()
    ordered_deps = []
    for dep in dependencies:
        if dep not in seen:
            seen.add(dep)
            ordered_deps.append(dep)
    
    # Add current directory last (after all dependencies)
    if current_abs not in seen:
        ordered_deps.append(current_abs)
    
    visited.remove(current_abs)
    return ordered_deps


def start_dependencies(dependency_paths: List[str], current_dir: str) -> None:
    """
    Start dependencies by recursively calling compose-init-up.py in each dependency directory.
    
    Args:
        dependency_paths: List of absolute paths to dependency directories
        current_dir: Current project directory (to skip when processing)
    """
    current_abs = os.path.abspath(current_dir)
    script_path = os.path.abspath(__file__)
    
    for dep_path in dependency_paths:
        if dep_path == current_abs:
            continue  # Skip self, will be started by main flow
        
        step(f"Starting dependency: {dep_path}")
        
        # Check if dependency is already running (basic heuristic)
        try:
            dep_env_file = os.path.join(dep_path, '.env')
            if os.path.isfile(dep_env_file):
                dep_env, _ = load_env_file(dep_env_file)
                # Try to detect if services are already running by checking common ports
                # This is a basic check - could be enhanced with actual service health checks
                info(f"Dependency {dep_path} has .env file, checking if already running...")
        except Exception:
            pass
        
        # Run compose-init-up.py in the dependency directory
        cmd = f"python3 {script_path} -d {dep_path} -y"
        try:
            info(f"Executing: {cmd}")
            result = subprocess.run(
                shlex.split(cmd),
                check=True,
                capture_output=True,
                text=True,
                timeout=300
            )
            info(f"Dependency {dep_path} started successfully")
            if result.stdout:
                info(f"Dependency output: {result.stdout.strip()}")
        except subprocess.CalledProcessError as e:
            error(f"Failed to start dependency {dep_path}: {e}\nStderr: {e.stderr}")
        except subprocess.TimeoutExpired:
            error(f"Timeout starting dependency {dep_path} (>5 minutes)")
        except Exception as e:
            error(f"Unexpected error starting dependency {dep_path}: {e}")



def generate_skeleton_toml(toml_out: str) -> None:
    """
    Dynamically generate a minimal `.env.toml` skeleton from CONFIG_VARIABLES.
    """
    # Determine a sensible default public FQDN by querying an external IP and
    # performing a reverse DNS lookup. This helps generate a more useful
    # skeleton for users who run the helper locally on a public host.
    def get_external_ip() -> str:
        # Allow overriding endpoints via environment variable
        urls = os.environ.get('COMPINIT_IP_DETECT_URLS')
        if urls:
            endpoints = [u.strip() for u in urls.split(',') if u.strip()]
        else:
            endpoints = DEFAULT_IP_DETECT_URLS

        for url in endpoints:
            try:
                with urllib.request.urlopen(url, timeout=5) as r:
                    ip = r.read().decode().strip()
                    # basic sanity check for IPv4/IPv6 or hostnames that may be returned
                    if ip:
                        return ip
            except Exception:
                continue
        return ''

    def reverse_dns(ip: str) -> str:
        try:
            name, _, _ = socket.gethostbyaddr(ip)
            return name.rstrip('.')
        except Exception:
            return ''

    def get_default_public_fqdn() -> str:
        # Prefer detected reverse DNS if available, otherwise fall back to
        # the configured CONFIG_VARIABLE default.
        ip = get_external_ip()
        if not ip:
            return next((v['default'] for v in CONFIG_VARIABLES if v['name'] == 'PUBLIC_FQDN'), 'example.local')
        rdns = reverse_dns(ip)
        if rdns:
            return rdns
        return ip

    detected_fqdn = get_default_public_fqdn()

    lines = [
        "# Project configuration in TOML format",
        "# This replaces .env.sample with better structure and metadata support",
        "",
        "[metadata]",
        f"project_name = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='LABEL_PROJECT_NAME'), 'my-project')}\"",
        f"env_tag = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='LABEL_ENV_TAG'), 'dev')}\"",
        f"label_prefix = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='LABEL_PREFIX'), 'example.com')}\"",
        "",
        "[control]",
    ]

    # Inject allowed-values metadata for control variables when available
    start_allowed = next((v.get('allowed_values') for v in CONFIG_VARIABLES if v['name']=='COMPINIT_COMPOSE_START_MODE'), None)
    if start_allowed:
        lines.append(f"# compose_start_mode allowed values: {' | '.join(start_allowed)}")
    else:
        lines.append("# compose_start_mode: how to behave when starting services that fail to start")

    lines.append(f"compose_start_mode = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='COMPINIT_COMPOSE_START_MODE'), 'abort-on-failure')}\"")

    reset_allowed = next((v.get('allowed_values') for v in CONFIG_VARIABLES if v['name']=='COMPINIT_RESET_BEFORE_START'), None)
    if reset_allowed:
        lines.append(f"# reset_before_start allowed values: {' | '.join(reset_allowed)}")
    else:
        lines.append("# reset_before_start: actions to perform before starting (comma-separated)")
        lines.append("#  options: none | containers | named-volumes | hostdirs | all")

    lines.extend([
        f"reset_before_start = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='COMPINIT_RESET_BEFORE_START'), 'none')}\"",
        f"image_check_enabled = {str(next((v['default'] for v in CONFIG_VARIABLES if v['name']=='COMPINIT_CHECK_IMAGE_ENABLED'), 'false')).lower()}",
        f"image_check_continue_on_error = {str(next((v['default'] for v in CONFIG_VARIABLES if v['name']=='COMPINIT_IMAGE_CHECK_CONTINUE_ON_ERROR'), '0'))}",
        "",
        "[infrastructure]",
        f"# Suggested default detected from your external IP (reverse DNS or IP): {detected_fqdn}",
        f"public_fqdn = \"{detected_fqdn}\"",
        f"public_tls_key_pem = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='PUBLIC_TLS_KEY_PEM'), '/etc/letsencrypt/live/$PUBLIC_FQDN/privkey.pem')}\"",
        f"public_tls_crt_pem = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='PUBLIC_TLS_CRT_PEM'), '/etc/letsencrypt/live/$PUBLIC_FQDN/fullchain.pem')}\"",
        "",
        "[health]",
        f"interval = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='HEALTHCHECK_INTERVAL'), '10s')}\"",
        f"timeout = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='HEALTHCHECK_TIMEOUT'), '5s')}\"",
        f"retries = {next((v['default'] for v in CONFIG_VARIABLES if v['name']=='HEALTHCHECK_RETRIES'), '3')}",
        f"start_period = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='HEALTHCHECK_START_PERIOD'), '30s')}\"",
        "",
        "[user]",
        f"uid = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='UID'), '$(id -u)')}\"",
        f"gid = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='GID'), '$(id -g)')}\"",
        "",
        "[dependencies]",
        "paths = []",
        "",
        "[hooks]",
        "pre_compose = []",
        "post_compose = []",
        "",
        "[variables]",
        "# Variables: type, default, description, allowed_values (if any)",
    ])

    # Emit detailed variable metadata entries for the TOML variables section
    for var in CONFIG_VARIABLES:
        name = var.get('name')
        vtype = var.get('type', 'string')
        default = var.get('default', '')
        desc = var.get('description', '')
        allowed = var.get('allowed_values')

        # Build a TOML inline table for metadata
        meta_parts = [f'type = "{vtype}"']
        if default is not None and default != '':
            # escape double quotes in default
            safe_def = str(default).replace('"', '\\"')
            meta_parts.append(f'default = "{safe_def}"')
        if desc:
            safe_desc = str(desc).replace('"', '\\"')
            meta_parts.append(f'description = "{safe_desc}"')
        if allowed:
            # format allowed values as TOML array of strings
            allowed_list = ', '.join([f'"{a}"' for a in allowed])
            meta_parts.append(f'allowed_values = [{allowed_list}]')

        meta_entry = f"{name} = {{{', '.join(meta_parts)}}}"
        lines.append(meta_entry)
    with open(toml_out, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    info(f"Generated skeleton TOML configuration: {toml_out}")



def generate_skeleton_env(sample_out: str) -> None:
    """
    Dynamically generate a minimal `.env.sample` skeleton from CONFIG_VARIABLES.
    """
    # Try to detect a sensible public_fqdn as default similar to TOML skeleton
    def detect_public_fqdn_for_env() -> str:
        # Reuse the same multi-endpoint detection as the TOML generator
        urls = os.environ.get('COMPINIT_IP_DETECT_URLS')
        if urls:
            endpoints = [u.strip() for u in urls.split(',') if u.strip()]
        else:
            endpoints = DEFAULT_IP_DETECT_URLS

        for url in endpoints:
            try:
                with urllib.request.urlopen(url, timeout=5) as r:
                    ip = r.read().decode().strip()
                try:
                    name, _, _ = socket.gethostbyaddr(ip)
                    return name.rstrip('.')
                except Exception:
                    return ip
            except Exception:
                continue
        return next((v['default'] for v in CONFIG_VARIABLES if v['name'] == 'PUBLIC_FQDN'), 'example.local')

    detected_env_fqdn = detect_public_fqdn_for_env()
    lines = [
        "# Generated minimal .env.sample by compose-init-up.py",
        "# Edit and extend with your service-specific variables.",
        "",
        "# compose_start_mode: how to behave when starting services that fail to start", 
        "#   abort-on-failure (default) | continue-on-error",
        "# reset_before_start: actions before start: none | containers | named-volumes | hostdirs | all",
    ]

    for var in CONFIG_VARIABLES:
        if var.get("description"):
            lines.append(f"# {var['description']}")
        if var['name'] == 'PUBLIC_FQDN':
            lines.append(f"{var['name']}={detected_env_fqdn}")
        else:
            lines.append(f"{var['name']}={var['default']}")
    lines.append("")
    with open(sample_out, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    info(f"Wrote skeleton .env.sample to {sample_out}")


def _load_toml_dict(path: str) -> dict:
    """Load TOML file into a dict. Uses tomllib (py3.11) or tomli fallback.

    Returns empty dict if file does not exist.
    """
    if not os.path.isfile(path):
        return {}
    try:
        import tomllib
    except Exception:
        try:
            import importlib
            tomllib = importlib.import_module('tomli')
        except Exception:
            warn("TOML parsing not available (tomllib/tomli). Cannot load: %s" % path)
            return {}
    try:
        with open(path, 'rb') as f:
            return tomllib.load(f)
    except Exception as e:
        warn(f"Failed to parse TOML file {path}: {e}")
        return {}


def _write_local_toml_variables(local_path: str, updates: Dict[str, str]) -> None:
    """Persist simple [variables] assignments into a per-project local TOML file.

    This file is intended to be git-ignored and only store non-sensitive
    overrides produced at runtime (hook outputs, local secrets). Only simple
    string values are supported here. Existing local file content will be
    merged (variables section updated); other sections are preserved if
    present.
    """
    # Load existing content if present
    data = _load_toml_dict(local_path)
    if not isinstance(data, dict):
        data = {}
    vars_section = data.get('variables', {}) or {}
    # Merge updates (overwriting existing keys)
    for k, v in updates.items():
        vars_section[k] = v
    data['variables'] = vars_section

    # Emit a minimal TOML file preserving top-level sections (simple serializer)
    try:
        with open(local_path, 'w', encoding='utf-8') as f:
            # Write metadata if present
            if 'metadata' in data and isinstance(data['metadata'], dict):
                f.write('[metadata]\n')
                for mk, mv in data['metadata'].items():
                    safe_mv = str(mv).replace('"', '\\"')
                    f.write(f'{mk} = "{safe_mv}"\n')
                f.write('\n')

            # Write variables section
            f.write('[variables]\n')
            for k, v in data['variables'].items():
                # Only simple string values supported here
                safe = str(v).replace('"', '\\"')
                f.write(f'{k} = "{safe}"\n')
            f.write('\n')
    except Exception as e:
        warn(f"Unable to persist local TOML overrides to {local_path}: {e}")


def _merge_global_project_local_toml(global_path: str, project_path: str, local_path: str, merged_out: str) -> None:
    """Merge global -> project -> local TOML dicts and write a merged TOML file.

    The merged file is a simple serialization that covers the common sections
    we need: metadata, control, infrastructure, health, user, registry, variables, hooks, dependencies.
    This is used at runtime so we don't have to rewrite the original project file.
    """
    # Support multiple global TOML files discovered by walking up the tree when
    # a basename like 'compose.config.global.toml' is provided. Apply
    # furthest-away-first (i.e., topmost parent first) so nearer project-level
    # values override more distant repo/global defaults.
    g = {}
    if global_path:
        # If a simple basename is given, search upward from cwd for matches
        if os.path.basename(global_path) == global_path:
            # walk from cwd up to filesystem root
            collected = []
            cur = Path(os.getcwd()).resolve()
            for parent in [cur] + list(cur.parents):
                cand = os.path.join(str(parent), global_path)
                if os.path.isfile(cand):
                    collected.append(cand)
            # Apply furthest-away-first: reverse collected so root-most first
            for path in reversed(collected):
                try:
                    d = _load_toml_dict(path)
                    if isinstance(d, dict):
                        # overlay into g
                        for k, v in d.items():
                            if k not in g:
                                g[k] = {}
                            if isinstance(v, dict):
                                g[k].update(v)
                except Exception:
                    continue
        else:
            g = _load_toml_dict(global_path) if global_path else {}
    p = _load_toml_dict(project_path) if project_path else {}
    l = _load_toml_dict(local_path) if local_path else {}

    def merged_section(name: str):
        out = {}
        for src in (g.get(name, {}), p.get(name, {}), l.get(name, {})):
            if isinstance(src, dict):
                out.update(src)
        return out

    merged = {}
    # Include metadata (take project metadata primarily)
    merged['metadata'] = merged_section('metadata') or p.get('metadata', g.get('metadata', {}))
    # Sections to merge
    for sec in ('control', 'infrastructure', 'health', 'user', 'registry'):
        sec_val = merged_section(sec)
        if sec_val:
            merged[sec] = sec_val

    # Dependencies and hooks
    deps = []
    for src in (g.get('dependencies', {}), p.get('dependencies', {}), l.get('dependencies', {})):
        if isinstance(src, list):
            deps.extend(src)
        elif isinstance(src, dict) and 'paths' in src:
            deps.extend(src.get('paths', []))
    if deps:
        merged['dependencies'] = {'paths': deps}

    # Hooks
    hooks = {}
    for k in ('pre_compose', 'post_compose'):
        combined = []
        for src in (g.get('hooks', {}).get(k, []), p.get('hooks', {}).get(k, []), l.get('hooks', {}).get(k, [])):
            if isinstance(src, list):
                combined.extend(src)
            elif isinstance(src, str):
                combined.append(src)
        if combined:
            hooks[k] = combined
    if hooks:
        merged['hooks'] = hooks

    # Merge variables (global -> project -> local)
    vars_merged = {}
    for src in (g.get('variables', {}), p.get('variables', {}), l.get('variables', {})):
        if isinstance(src, dict):
            for k, v in src.items():
                vars_merged[k] = v
    if vars_merged:
        merged['variables'] = vars_merged

    # Emit merged TOML (minimal serializer)
    try:
        with open(merged_out, 'w', encoding='utf-8') as f:
            # Metadata
            if 'metadata' in merged and isinstance(merged['metadata'], dict):
                f.write('[metadata]\n')
                for mk, mv in merged['metadata'].items():
                    safe_mv = str(mv).replace('"', '\\"')
                    f.write(f'{mk} = "{safe_mv}"\n')
                f.write('\n')

            for sec in ('control', 'infrastructure', 'health', 'user', 'registry'):
                if sec in merged:
                    f.write(f'[{sec}]\n')
                    for k, v in merged[sec].items():
                        if isinstance(v, bool):
                            val = 'true' if v else 'false'
                            f.write(f'{k} = {val}\n')
                        else:
                            safe = str(v).replace('"', '\\"')
                            f.write(f'{k} = "{safe}"\n')
                    f.write('\n')

            if 'dependencies' in merged and isinstance(merged['dependencies'], dict):
                f.write('[dependencies]\n')
                paths = merged['dependencies'].get('paths', [])
                if isinstance(paths, list):
                    f.write('paths = [')
                    f.write(', '.join([f'"{p}"' for p in paths]))
                    f.write(']\n\n')

            if 'hooks' in merged and isinstance(merged['hooks'], dict):
                f.write('[hooks]\n')
                for k, arr in merged['hooks'].items():
                    if isinstance(arr, list):
                        f.write(f'{k} = [')
                        f.write(', '.join([f'"{x}"' for x in arr]))
                        f.write(']\n')
                f.write('\n')

            if 'variables' in merged and isinstance(merged['variables'], dict):
                f.write('[variables]\n')
                for k, v in merged['variables'].items():
                    if isinstance(v, dict) and 'value' in v:
                        safe = str(v['value']).replace('"', '\\"')
                        f.write(f'{k} = "{safe}"\n')
                    elif isinstance(v, str):
                        safe = v.replace('"', '\\"')
                        f.write(f'{k} = "{safe}"\n')
                    else:
                        # Fallback to string representation
                        safe = str(v).replace('"', '\\"')
                        f.write(f'{k} = "{safe}"\n')
                f.write('\n')
    except Exception as e:
        warn(f"Failed to write merged TOML to {merged_out}: {e}")


def migrate_env_sample_to_toml(sample_path: str, toml_out: str) -> None:
    """
    Migrate a legacy .env.sample (or .env) to a simple TOML configuration.

    The migration attempts to preserve variable names and values. It will
    heuristically map variable names to TOML variable metadata:
      - *_PASSWORD -> type=password
      - *_PASSWORD_DEFERED -> source=deferred
      - *_TOKEN_INTERNAL -> type=token, source=internal
      - *_TOKEN_EXTERNAL -> type=token, source=external
      - other -> type=string

    This produces a conservative TOML suitable as a starting point for
    manual editing.
    """
    if not os.path.isfile(sample_path):
        error(f"Cannot migrate: sample file not found: {sample_path}")

    env_vars, _ = load_env_file(sample_path)

    lines = [
        "# Migrated TOML configuration generated from legacy .env.sample",
        "# Review and adjust types/sources as needed",
        "",
        "[metadata]",
        f"project_name = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='LABEL_PROJECT_NAME'), 'my-project')}\"",
        f"env_tag = \"{next((v['default'] for v in CONFIG_VARIABLES if v['name']=='LABEL_ENV_TAG'), 'dev')}\"",
        "",
        "[variables]",
    ]

    def var_entry(k: str, v: str) -> str:
        # Decide type/source heuristics
        entry = None
        # Accept both historical misspelling 'DEFERED' and correct 'DEFERRED'
        if k.endswith('_PASSWORD_DEFERED') or k.endswith('_PASSWORD_DEFERRED'):
            entry = f'{k} = {{ type = "password", source = "deferred" }}'
        elif k.endswith('_PASSWORD'):
            entry = f'{k} = {{ type = "password", value = "{v}" }}'
        elif k.endswith('_TOKEN_INTERNAL'):
            entry = f'{k} = {{ type = "token", source = "internal", value = "{v}" }}'
        elif k.endswith('_TOKEN_EXTERNAL'):
            entry = f'{k} = {{ type = "token", source = "external", value = "{v}" }}'
        elif '_TOKEN_' in k or k.endswith('_TOKEN'):
            entry = f'{k} = {{ type = "token", value = "{v}" }}'
        else:
            # default to string
            # Escape double quotes
            safe = v.replace('"', '\\"') if isinstance(v, str) else str(v)
            entry = f'{k} = {{ type = "string", value = "{safe}" }}'
        return entry

    for k in sorted(env_vars.keys()):
        v = env_vars.get(k, '')
        lines.append(var_entry(k, v))

    with open(toml_out, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines) + '\n')

    info(f"Migrated {sample_path} -> {toml_out}")


# Legacy mapping helper removed: scripts now require canonical variable names only.

def run(cmd: str, check: bool = True, capture_output: bool = False, 
        env: Optional[Dict[str, str]] = None, suppress_output: bool = False) -> Optional[str]:
    """
    Run a shell command with improved error handling.
    
    Args:
        cmd: Command to execute
        check: Whether to raise exception on non-zero exit codes
        capture_output: Whether to capture and return stdout
        env: Environment variables for the command
        suppress_output: Whether to suppress logging the command
        
    Returns:
        Command stdout if capture_output=True, None otherwise
        
    Raises:
        subprocess.CalledProcessError: If command fails and check=True
    """
    if not suppress_output:
        info(f"Running: {cmd}")
    
    try:
        result = subprocess.run(
            shlex.split(cmd), 
            check=check, 
            capture_output=capture_output, 
            text=True, 
            env=env,
            timeout=300  # 5 minute timeout to prevent hanging
        )
        
        if capture_output:
            return result.stdout
        return None
        
    except subprocess.TimeoutExpired:
        error(f"Command timed out after 5 minutes: {cmd}")
    except subprocess.CalledProcessError as e:
        if check:
            error(f"Command failed with exit code {e.returncode}: {cmd}\nStderr: {e.stderr}")
        else:
            warn(f"Command failed with exit code {e.returncode}: {cmd}")
            return None
    except FileNotFoundError:
        error(f"Command not found: {cmd.split()[0]}")
    except Exception as e:
        error(f"Unexpected error running command '{cmd}': {e}")



def run_hooks(hook_type: str, env_vars: Dict[str, str], project_config: Optional[ProjectConfig] = None) -> Dict[str, str]:
    """Execute pre/post compose hooks and return environment variable changes.
    
    Args:
        hook_type: Either 'PRE_COMPOSE' or 'POST_COMPOSE'
        env_vars: Current environment variables
        
    Returns:
        Dict of environment variable changes from hooks
    """
    # Determine ordered hook list. Prefer project_config hooks when provided
    paths = []
    if project_config:
        hooks_list = project_config.pre_compose_hooks if hook_type == 'PRE_COMPOSE' else project_config.post_compose_hooks
        if hooks_list:
            for hc in hooks_list:
                try:
                    paths.append(hc.script)
                except Exception:
                    # Skip malformed HookConfig entries
                    continue

    # Fallback to environment variable listing if no project_config hooks provided
    if not paths:
        hook_var = f'COMPINIT_HOOK_{hook_type}'
        hook_paths = env_vars.get(hook_var, '').strip()
        if not hook_paths:
            return {}
        paths = [p.strip() for p in hook_paths.split(',') if p.strip()]

    # Normalized metadata map: name -> {'value':..., 'persist': 'auto'|'none'|'env'|'project', 'sensitive': bool, 'source_hook': str}
    meta_changes: Dict[str, Dict[str, Any]] = {}

    for hook_path in paths:
        if not os.path.isfile(hook_path):
            warn(f"Hook file not found: {hook_path}")
            continue

        try:
            info(f"Executing {hook_type.lower()} hook: {hook_path}")

            # Import hook module dynamically
            import importlib.util
            spec = importlib.util.spec_from_file_location("hook_module", hook_path)
            if not spec or not spec.loader:
                warn(f"Could not load hook module: {hook_path}")
                continue

            hook_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(hook_module)

            # Require class-based hook with exact class name, e.g. PreComposeHook or PostComposeHook
            hook_class_name = f'{hook_type.title().replace("_", "")}Hook'
            if hasattr(hook_module, hook_class_name):
                hook_class = getattr(hook_module, hook_class_name)
                # Determine restricted environment to give to hook (only env_reads if declared)
                hook_env = dict(os.environ)
                matching = None
                if project_config:
                    for hc in (project_config.pre_compose_hooks if hook_type == 'PRE_COMPOSE' else project_config.post_compose_hooks):
                        try:
                            if os.path.abspath(hc.script) == os.path.abspath(hook_path) or os.path.basename(hc.script) == os.path.basename(hook_path):
                                matching = hc
                                break
                        except Exception:
                            continue
                    if matching and getattr(matching, 'env_reads', None):
                        hook_env = {k: os.environ.get(k, '') for k in matching.env_reads}

                # Instantiate hook. Try no-arg constructor first, then fallback to constructor accepting env
                try:
                    hook_instance = hook_class()
                except TypeError:
                    try:
                        hook_instance = hook_class(hook_env)
                    except Exception:
                        # last-resort, instantiate without args and hope for the best
                        hook_instance = hook_class()

                # Execute hook.run with the limited hook_env (if available)
                try:
                    hook_changes = hook_instance.run(hook_env) if hasattr(hook_instance, 'run') else {}
                except Exception as e:
                    warn(f"Hook {hook_path} raised an exception during run(): {e}")
                    hook_changes = {}

                if hook_changes:
                    # Normalize different return structures into meta_changes
                    # Supported return shapes:
                    # - dict of name -> value (legacy)
                    # - dict of name -> {value:, persist:, sensitive:}
                    # - list of dicts [{name:, value:, persist:, sensitive:}, ...]
                    try:
                        if isinstance(hook_changes, dict):
                                    for k, raw in hook_changes.items():
                                        if isinstance(raw, dict):
                                            val = raw.get('value')
                                            persist = raw.get('persist', 'auto')
                                            sensitive = bool(raw.get('sensitive', False))
                                        else:
                                            val = raw
                                            persist = 'auto'
                                            sensitive = False
                                        # If key already present from earlier hooks, later hooks override
                                        meta_changes[k] = {'value': val, 'persist': persist, 'sensitive': sensitive, 'source_hook': hook_path}
                        elif isinstance(hook_changes, list):
                            for item in hook_changes:
                                if not isinstance(item, dict):
                                    continue
                                name = item.get('name') or item.get('var') or item.get('key')
                                if not name:
                                    continue
                                val = item.get('value')
                                persist = item.get('persist', 'auto')
                                sensitive = bool(item.get('sensitive', False))
                                meta_changes[name] = {'value': val, 'persist': persist, 'sensitive': sensitive, 'source_hook': hook_path}
                        else:
                            warn(f"Hook {hook_path} returned unsupported type {type(hook_changes)}; expected dict or list")
                    except Exception as e:
                        warn(f"Failed to normalize hook result from {hook_path}: {e}")
                    info(f"Hook {hook_path} returned {len(hook_changes) if hasattr(hook_changes, '__len__') else '1'} changes (normalized to {len(meta_changes)} meta entries)")
            else:
                warn(f"Hook {hook_path} does not define required class {hook_class_name}; only class-based hooks are supported")

        except Exception as e:
            warn(f"Error executing hook {hook_path}: {e}")
            continue

    # Post-validate against declared HookConfig env_reads/env_writes/env_modifies/env_exports when available
    if project_config and meta_changes:
        hooks_list = project_config.pre_compose_hooks if hook_type == 'PRE_COMPOSE' else project_config.post_compose_hooks
        for hc in (hooks_list or []):
            try:
                hc_script_abs = os.path.abspath(hc.script)
            except Exception:
                hc_script_abs = hc.script
            for var_name, meta in list(meta_changes.items()):
                # Only validate entries that this hook produced
                try:
                    if not meta.get('source_hook'):
                        continue
                    if not (os.path.abspath(meta.get('source_hook')) == os.path.abspath(hc.script) or os.path.basename(meta.get('source_hook')) == os.path.basename(hc.script)):
                        continue
                except Exception:
                    continue
                # Allowed writes are env_writes, env_modifies, and env_exports
                allowed = set((getattr(hc, 'env_writes', []) or []) + (getattr(hc, 'env_modifies', []) or []) + (getattr(hc, 'env_exports', []) or []))
                if allowed and var_name not in allowed:
                    warn(f"Hook {hc.script} attempted to return variable '{var_name}' which is not declared in env_writes/env_modifies/env_exports ({', '.join(sorted(allowed))})")

    return meta_changes

def usage_notes() -> str:
    """
    Dynamically render usage notes and variable documentation from CONFIG_VARIABLES.
    """
    notes = [
        "Extended usage notes for compose-init-up.py\n",
        "- TOML-first: prefer a .env.toml configuration for new projects (recommended).",
        "- Legacy support is opt-in: pass --env-legacy to allow parsing legacy .env.sample/.env files. Without --env-legacy, legacy files will be refused and you will be offered a migration to TOML.",
        "- Hooks: only class-based Python hooks are supported. Define PreComposeHook and PostComposeHook classes with a run(env) method. Legacy stdout/main-based hooks are not supported.",
        "- Use COMPINIT_DEPENDENCIES to list dependency directories (comma-separated) that must be started first.",
        "- Use COMPINIT_IMAGE_CHECK_ENABLED to enable/disable image availability checks.",
        "- Use --init-skel-toml to generate a skeleton .env.sample.toml, --init-skel-env for legacy .env.sample.",
        "- COMPINIT_MYREGISTRY_URL can be set to a local registry mirror to speed pulls.",
        "- COMPINIT_ASSUME_YES=1 will run non-interactively and skip prompts (useful in CI).",
        "- COMPINIT_EXTERNAL_STRICT=1 will make missing EXTERNAL tokens a hard error in non-interactive mode.",
        "- COMPINIT_ENABLE_ENV_EXPANSION controls whether $VAR and $(cmd) expansions are applied at runtime; by default generated .env contains literal expanded values where appropriate.",
        "\n- New: prefer COMPINIT_STRICT_<NAME> variables for per-feature strict toggles (e.g. COMPINIT_STRICT_EXTERNAL, COMPINIT_STRICT_EXPANSION). Do NOT implement runtime fallbacks or compatibility shims that check for the legacy COMPINIT_<NAME>_STRICT variables. When changing strict-related behavior, perform a repository-wide refactor: update code, tests, documentation, and CI so only the canonical COMPINIT_STRICT_<NAME> names are used.",
        "\nDocker Hub rate-limiting and best practices:",
        "    See: https://docs.docker.com/docker-hub/usage/\n",
        "\nCanonical config variables:\n"
    ]
    for var in CONFIG_VARIABLES:
        notes.append(f"- {var['name']}: {var['description']} (default: {var['default']})")
    notes.append("\nKEEP IN SYNC: When changing the list above, also update the .env.sample and docs that mention COMPINIT_* variable names.")
    return '\n'.join(notes)


def check_myregistry(url: str, env: Dict[str, str]) -> Tuple[bool, str]:
    """
    Check if COMPINIT_MYREGISTRY_URL is reachable and optionally test credentials.
    Returns (ok, message). Verbose output helps debugging registry auth and reachability.
    """
    if not url:
        return False, 'No registry URL provided'

    try:
        info(f"Checking registry URL: {url}")

        # Build a test image reference that targets the registry host if possible.
        # If user provided a full registry URL like myregistry.local:5000, use that as host for a small image.
        # Otherwise fall back to a small public image to at least verify Docker can reach registries.
        user_host = url.strip().rstrip('/')
        test_image = os.environ.get('COMPINIT_MYREGISTRY_TEST_IMAGE', None)
        if test_image:
            target_image = test_image
        else:
            # Use an extremely small manifest target; prefer 'alpine:3.18' or busybox
            target_image = f"{user_host}/busybox:1" if ':' in user_host or '/' in user_host else 'busybox:1'

        info(f"Attempting docker manifest inspect for: {target_image}")

        # Support optional basic auth via COMPINIT_MYREGISTRY_USER / COMPINIT_MYREGISTRY_PASSWORD
        auth_user = os.environ.get('COMPINIT_MYREGISTRY_USER') or os.environ.get('COMPINIT_MYREGISTRY_USERNAME')
        auth_pw = os.environ.get('COMPINIT_MYREGISTRY_PASSWORD') or os.environ.get('COMPINIT_MYREGISTRY_TOKEN')

        # Build docker CLI command; avoid --insecure by default, allow operator to set DOCKER CLI configs.
        cmd = ["docker", "manifest", "inspect", target_image]

        # If auth is provided, try to login temporarily to enable authenticated inspect (best-effort).
        logged_in = False
        login_cmd = None
        if auth_user and auth_pw:
            try:
                info("Attempting docker login to provided registry for authenticated check (credentials from env)")
                # docker login requires host only; extract host portion
                registry_host = user_host
                login_cmd = ["docker", "login", registry_host, "-u", auth_user, "--password-stdin"]
                p = subprocess.run(login_cmd, input=auth_pw, text=True, capture_output=True, timeout=20)
                if p.returncode == 0:
                    logged_in = True
                    info("Docker login succeeded for registry host")
                else:
                    warn(f"Docker login returned non-zero: {p.stderr or p.stdout}")
            except Exception as e:
                warn(f"Docker login attempt failed: {e}")

        try:
            res = subprocess.run(cmd, capture_output=True, text=True, timeout=20)
            out = (res.stdout or '').strip()
            err = (res.stderr or '').strip()
            if res.returncode == 0:
                return True, 'Registry and credentials appear to be working (manifest inspect OK)'
            else:
                # Detect Docker Hub rate-limit hints
                lowered = (err + out).lower()
                if 'toomanyrequests' in lowered or 'rate limit' in lowered:
                    return False, 'Manifest inspect failed: Docker Hub rate limit or too many requests. See https://docs.docker.com/docker-hub/download-rate-limit/'
                # Generic failure
                msg = err or out or f'exit code {res.returncode}'
                return False, f'Registry manifest inspect failed: {msg[:400]}'
        finally:
            # If we logged in for the check, attempt to logout to avoid leaving credentials in docker config
            if logged_in and login_cmd:
                try:
                    subprocess.run(["docker", "logout", user_host], capture_output=True, text=True, timeout=10)
                except Exception:
                    pass

    except subprocess.TimeoutExpired:
        return False, 'Timeout while checking registry URL'
    except Exception as e:
        return False, f'Error while checking registry URL: {e}'

def reset_state(env: Dict[str, str], reset_flags: str, env_file: str) -> bool:
    """
    Reset Docker state based on configuration flags.
    Returns True if env-active was deleted and script should restart.
    """
    flags = [f.strip() for f in reset_flags.split(',') if f.strip() and f.strip() != 'none']
    
    if not flags:
        step("Perform reset actions before start: skipping")
        return False
    
    step("Perform reset actions before start")
    
    # 'all' is shorthand for common reset actions
    if 'all' in flags:
        flags = ['containers', 'named-volumes', 'hostdirs']
    
    for action in flags:
        try:
            if action == 'containers':
                info("Reset: removing containers/networks...")
                run("docker compose down --remove-orphans", check=False)
            elif action == 'named-volumes':
                info("Reset: removing named volumes and containers...")
                run("docker compose down -v --remove-orphans", check=False)
            elif action == 'networks':
                info("Reset: pruning unused docker networks...")
                run("docker network prune -f", check=False)
            elif action == 'hostdirs':
                info("Reset: removing host-mounted data directories referenced by *_HOSTDIR_* variables...")
                for k, v in env.items():
                    if not v:
                        continue
                    if ('_HOSTDIR_' in k) or k.endswith('_HOSTDIR') or k.endswith('_HOSTDIRS') or ('HOSTDIR' in k):
                        try:
                            # Normalize path: expand env vars, user, and make absolute relative to current cwd
                            resolved = os.path.expanduser(os.path.expandvars(v))
                            if not os.path.isabs(resolved):
                                resolved = os.path.abspath(resolved)
                            if os.path.isdir(resolved):
                                info(f"Removing host data dir: {resolved}")
                                shutil.rmtree(resolved, ignore_errors=True)
                            else:
                                info(f"Host data dir not present (skipping): {resolved}")
                        except Exception as e:
                            warn(f"Failed to remove directory {v}: {e}")
            elif action == 'env-active':
                info(f"Reset: deleting active environment file {env_file} ...")
                try:
                    os.remove(env_file)
                    info(f"Deleted {env_file}. Restarting script...")
                    return True
                except Exception as e:
                    error(f"Failed to delete {env_file}: {e}")
            else:
                warn(f"Unknown reset action: {action}")
        except Exception as e:
            warn(f"Error during reset action '{action}': {e}")
    
    return False

def check_compose_yaml(compose_file: str, env_vars: Dict[str, str], env_file: str) -> str:
    """
    Validate Docker Compose configuration and check environment variable usage.
    
    Args:
        compose_file: Path to docker-compose.yml file
        env_vars: Environment variables dictionary
        env_file: Path to the environment file
        
    Returns:
        The docker-compose config YAML output
        
    Raises:
        SystemExit: If validation fails or required variables are missing
    """
    step(f"Validating docker compose configuration for {compose_file}")
    
    if not os.path.isfile(compose_file):
        error(f"Docker compose file '{compose_file}' not found")
    
    # Run docker compose config and parse YAML
    try:
        config_yaml = run(f"docker compose -f {compose_file} config", 
                         capture_output=True, env=env_vars, suppress_output=True)
        if not config_yaml:
            error("docker-compose config returned empty output")
    except Exception as e:
        error(f"docker-compose validation failed: {e}")
    
    # Parse YAML for variable usage analysis
    referenced_vars = set()
    
    try:
        # Read the raw compose file to find ${VAR} references
        with open(compose_file, 'r', encoding='utf-8') as f:
            raw_compose_content = f.read()
        
        # Find all ${VAR} and ${VAR:-default} references in the raw compose file
        for match in re.finditer(r'\${([A-Za-z_][A-Za-z0-9_]*)[^}]*?}', raw_compose_content):
            referenced_vars.add(match.group(1))
            
    except Exception as e:
        warn(f"Error analyzing variable references: {e}")
        # Fallback to the processed config if raw file reading fails
        try:
            for match in re.finditer(r'\${([A-Za-z_][A-Za-z0-9_]*)}', config_yaml):
                referenced_vars.add(match.group(1))
        except Exception as e2:
            warn(f"Error analyzing processed config: {e2}")
    
    # Find all env vars defined
    defined_vars = set(env_vars.keys())
    
    # Warn for defined but unused variables (not necessarily an error)
    unused = defined_vars - referenced_vars
    for var in sorted(unused):
        if var.startswith('COMPINIT_'):
            continue  # Exclude script control variables from warning
        warn(f"Variable defined in {env_file} but not used in compose YAML: {var}")
    
    # Error for referenced but undefined variables (this is a real problem)
    # Only error if the variable is referenced in an active (uncommented) line, warn if only in commented lines
    missing = referenced_vars - defined_vars
    if missing:
        # Read compose file lines for context
        with open(compose_file, 'r', encoding='utf-8') as f:
            compose_lines = f.readlines()
        for var in sorted(missing):
            found_in_active = False
            found_in_commented = False
            for line in compose_lines:
                # Look for ${VAR} or ${VAR:-...} in the line
                if re.search(r'\${' + re.escape(var) + r'([}:]|:-)', line):
                    if line.lstrip().startswith('#') or line.lstrip().startswith('//'):
                        found_in_commented = True
                    else:
                        found_in_active = True
                        break
            if found_in_active:
                error(f"Variable referenced in compose YAML but not defined in {env_file}: {var}", show_stacktrace=False)
            elif found_in_commented:
                warn(f"Variable referenced in commented line in compose YAML but not defined in {env_file}: {var}")
    
    return config_yaml

def get_images_from_compose(compose_file: str, env_vars: Dict[str, str]) -> List[str]:
    """
    Extract Docker image names from compose configuration.
    
    Args:
        compose_file: Path to docker-compose.yml file
        env_vars: Environment variables for config rendering
        
    Returns:
        List of Docker image names referenced in the compose file
    """
    try:
        config = run(f"docker compose -f {compose_file} config", 
                    capture_output=True, env=env_vars, suppress_output=True)
        if not config:
            warn("docker-compose config returned empty output for image extraction")
            return []
    except Exception as e:
        warn(f"Failed to get compose config for image extraction: {e}")
        return []
    
    images = set()
    for line in config.splitlines():
        m = re.match(r'\s*image:\s*([\w./:-]+)', line)
        if m:
            images.add(m.group(1))
    
    return list(images)

def check_for_image_updates(images: List[str]) -> None:
    """
    Check for major version updates of used images.
    
    This function examines Docker images to see if there are newer major versions
    available. For example, if using 'postgres:17', it will check if 'postgres:18'
    or later versions are available.
    
    Args:
        images: List of Docker image names to check for updates
    """
    step("Checking for potential image updates...")
    
    found_any_updates = False
    
    for image in images:
        try:
            # Skip local images
            if ':local' in image or ('/' not in image and ':' not in image):
                continue
                
            # Parse image name and tag
            if ':' in image:
                base_image, current_tag = image.rsplit(':', 1)
            else:
                base_image = image
                current_tag = 'latest'
            
            # Skip if current tag is 'latest' or not numeric
            if current_tag == 'latest' or not re.match(r'^\d+', current_tag):
                continue
            
            # Extract major version number
            version_match = re.match(r'^(\d+)', current_tag)
            if not version_match:
                continue
                
            current_major = int(version_match.group(1))
            
            # Check for next major version (simple heuristic)
            for next_major in range(current_major + 1, current_major + 3):
                next_tag = str(next_major)
                next_image = f"{base_image}:{next_tag}"
                
                try:
                    # Check if the newer version exists
                    if os.environ.get('DST_DEBUG_UPDATES') == '1':
                        info(f"Checking for update: {image} -> {next_image}")
                        
                    result = subprocess.run(
                        ["docker", "manifest", "inspect", next_image],
                        capture_output=True,
                        text=True,
                        timeout=15  # Increased timeout
                    )
                    
                    if result.returncode == 0:
                        warn(f"Image update available: '{image}' -> '{next_image}'")
                        found_any_updates = True
                        break  # Found an update, don't check further
                    elif os.environ.get('DST_DEBUG_UPDATES') == '1':
                        info(f"No update found for {next_image}")
                        
                except subprocess.TimeoutExpired:
                    if os.environ.get('DST_DEBUG_UPDATES') == '1':
                        warn(f"Timeout checking {next_image}")
                    pass
                except Exception as e:
                    if os.environ.get('DST_DEBUG_UPDATES') == '1':
                        warn(f"Error checking {next_image}: {e}")
                    pass
                    
        except Exception as e:
            # Don't fail the entire process for update checking errors
            pass
    
    if not found_any_updates:
        info("No major image updates detected")

def check_images_parallel(images: List[str]) -> Tuple[List[str], List[str]]:
    """
    Check availability of Docker images in parallel.
    
    Args:
        images: List of Docker image names to check
        
    Returns:
        Tuple of (available_images, missing_images)
    """
    if not images:
        return [], []
    
    missing = []
    available = []
    q = queue.Queue()
    
    # Read control variable from environment (canonical name only)
    continue_on_error = os.environ.get('COMPINIT_IMAGE_CHECK_CONTINUE_ON_ERROR', '0') == '1'
    
    def worker(image: str) -> None:
        """Worker function to check a single image."""
        try:
            result = subprocess.run(
                ["docker", "manifest", "inspect", image], 
                capture_output=True, 
                text=True,
                timeout=30  # 30 second timeout per image
            )
            if result.returncode == 0:
                info(f"Image available: {image}")
                q.put((image, True, None))
            else:
                # Try to provide more specific error information
                error_msg = result.stderr.strip() if result.stderr else f"Exit code {result.returncode}"
                # Docker Hub rate limit: treat as warning, not error
                if 'toomanyrequests' in error_msg.lower():
                    warn(f"Image NOT available: {image} ({error_msg}) [rate limit: inconclusive, continuing]")
                    q.put((image, True, None))
                # Local image denied/unauthorized: treat as error unless continue_on_error
                elif (':local' in image or image.endswith(':local')) and (
                    'denied' in error_msg.lower() or 'unauthorized' in error_msg.lower()):
                    msg = f"Image NOT available: {image} ({error_msg}) [local image access denied]"
                    if continue_on_error:
                        warn(msg + " [continuing due to COMPINIT_IMAGE_CHECK_CONTINUE_ON_ERROR=1]")
                        q.put((image, True, None))
                    else:
                        q.put((image, False, msg, 'fatal'))
                else:
                    warn(f"Image NOT available: {image} ({error_msg})")
                    q.put((image, False, error_msg))
        except subprocess.TimeoutExpired:
            error_msg = "Timeout after 30 seconds"
            warn(f"Timeout checking image: {image}")
            q.put((image, False, error_msg))
        except Exception as e:
            error_msg = str(e)
            warn(f"Error checking image {image}: {e}")
            q.put((image, False, error_msg))
    
    # Start worker threads
    threads = []
    for image in images:
        t = threading.Thread(target=worker, args=(image,))
        t.daemon = True  # Don't prevent program exit
        t.start()
        threads.append(t)
    
    # Wait for all threads to complete
    for t in threads:
        try:
            t.join(timeout=60)  # Wait up to 60 seconds for each thread
        except Exception as e:
            warn(f"Error waiting for image check thread: {e}")
    
    # Collect results
    error_details = {}
    fatal_error = None
    while not q.empty():
        try:
            item = q.get_nowait()
            if len(item) == 4 and item[3] == 'fatal':
                # Fatal error from worker
                fatal_error = item[2]
                continue
            image, ok, error_msg = item[:3]
            if ok:
                available.append(image)
            else:
                missing.append(image)
                if error_msg:
                    error_details[image] = error_msg
        except queue.Empty:
            break
    
    # Log detailed error information if requested
    if error_details and os.environ.get('DST_DEBUG_IMAGES') == '1':
        step("Detailed image check errors:")
        for image, error in error_details.items():
            info(f"  {image}: {error}")
    
    # Use the global error function
    if fatal_error:
        globals()['error'](fatal_error)

    return available, missing

def create_hostdirs_from_env(env_vars: Dict[str, str]) -> None:
    """
    Create host directories specified in environment variables.
    
    Creates directories for variables matching *_HOSTDIR_* pattern and sets
    appropriate permissions based on UID/GID environment variables.
    
    Args:
        env_vars: Environment variables dictionary
    """
    for var, value in env_vars.items():
        if '_HOSTDIR_' in var or var.endswith('_HOSTDIR') or var.endswith('_HOSTDIRS'):
            if not value:
                continue
            try:
                uid = int(env_vars.get('UID', '0'))
                gid = int(env_vars.get('GID', '0'))
                created = False
                fixed = False
                if os.path.exists(value):
                    st = os.stat(value)
                    # Check ownership
                    if st.st_uid != uid or st.st_gid != gid:
                        warn(f"Ownership of {value} is {st.st_uid}:{st.st_gid}, expected {uid}:{gid}. Attempting to fix...")
                        try:
                            os.chown(value, uid, gid)
                            fixed = True
                        except Exception as e:
                            warn(f"Could not set ownership for {value}: {e}")
                    # Check permissions
                    mode = st.st_mode & 0o777
                    if mode != 0o770:
                        warn(f"Permissions of {value} are {oct(mode)}, expected 0o770. Attempting to fix...")
                        try:
                            os.chmod(value, 0o770)
                            fixed = True
                        except Exception as e:
                            warn(f"Could not set permissions for {value}: {e}")
                    # Check writability for current user
                    if not os.access(value, os.W_OK):
                        error(f"Directory {value} is not writable for the current user.")
                    if fixed:
                        info(f"Setting up directory {value} for variable {var}")
                    else:
                        info(f"Directory {value} for variable {var}: check OK")
                else:
                    os.makedirs(value, exist_ok=True)
                    os.chown(value, uid, gid)
                    os.chmod(value, 0o770)
                    info(f"Setting up directory {value} for variable {var}")
            except Exception as e:
                error(f"Failed to set up directory {value}: {e}")

def wait_for_services_health(compose_file: str, timeout: int = 300, interval: int = 5) -> bool:
    """
    Poll container health status until all are healthy or timeout.
    
    Args:
        compose_file: Path to docker-compose.yml
        timeout: Maximum seconds to wait
        interval: Seconds between checks
        
    Returns:
        True if all services became healthy, False otherwise
    """
    import time
    
    info(f"Waiting for services to become healthy (timeout: {timeout}s)...")
    start_time = time.time()
    
    while True:
        elapsed = time.time() - start_time
        if elapsed > timeout:
            warn(f"Timeout waiting for services to become healthy after {timeout}s")
            return False
        
        try:
            # Get container status as JSON
            result = subprocess.run(
                ['docker', 'compose', '-f', compose_file, 'ps', '--format', 'json'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode != 0:
                warn(f"Failed to query container status: {result.stderr}")
                time.sleep(interval)
                continue
            
            if not result.stdout.strip():
                warn("No containers found")
                time.sleep(interval)
                continue
            
            # Parse container statuses
            all_healthy = True
            any_unhealthy = False
            containers_info = []
            
            for line in result.stdout.strip().split('\n'):
                if not line.strip():
                    continue
                    
                try:
                    container = json.loads(line)
                    name = container.get('Service', 'unknown')
                    state = container.get('State', 'unknown')
                    health = container.get('Health', '')
                    
                    containers_info.append((name, state, health))
                    
                    # Check health status
                    if health:
                        if 'healthy' in health.lower():
                            continue  # This container is healthy
                        elif 'unhealthy' in health.lower():
                            warn(f"Container {name} is unhealthy")
                            any_unhealthy = True
                            all_healthy = False
                        elif 'starting' in health.lower():
                            all_healthy = False  # Still starting
                        else:
                            warn(f"Container {name} has unknown health status: {health}")
                            all_healthy = False
                    else:
                        # No health check defined - check if running
                        if state == 'running':
                            continue  # Treat as healthy
                        elif state == 'exited':
                            warn(f"Container {name} has exited")
                            any_unhealthy = True
                            all_healthy = False
                        else:
                            all_healthy = False  # Still starting or unknown
                            
                except json.JSONDecodeError:
                    warn(f"Failed to parse container status: {line[:100]}")
                    all_healthy = False
                    continue
            
            # Report status
            if all_healthy:
                info("All services are healthy!")
                for name, state, health in containers_info:
                    status = health if health else state
                    info(f"   {name}: {status}")
                return True
            
            if any_unhealthy:
                error_msg = "Some services are unhealthy:"
                for name, state, health in containers_info:
                    status = health if health else state
                    if 'unhealthy' in status.lower() or state == 'exited':
                        error_msg += f"\n   {name}: {status}"
                warn(error_msg)
                return False
            
            # Still waiting for services to start
            info(f"Waiting... ({int(elapsed)}s elapsed)")
            for name, state, health in containers_info:
                status = health if health else state
                info(f"  - {name}: {status}")
            
        except subprocess.TimeoutExpired:
            warn("Docker ps command timed out")
        except Exception as e:
            warn(f"Error checking container health: {e}")
        
        time.sleep(interval)

def start_compose(compose_file: str, mode: str, expanded_env: Dict[str, str], env_file: str) -> None:
    """
    Start Docker Compose services with the specified mode.
    
    Args:
        compose_file: Path to the docker-compose.yml file
        mode: Start mode - one of: detached, abort-on-failure, foreground
        expanded_env: Environment variables with expansions applied
        env_file: Path to the environment file for persistence
        
    Raises:
        SystemExit: If invalid mode specified or docker compose fails
        
    Modes:
        - detached: Start containers in background and return immediately
        - abort-on-failure: Start containers in background, wait for health, exit on success/failure
        - foreground: Start containers in foreground (blocks until Ctrl+C)
    """
    step("Starting containers with Docker Compose")
    
    valid_modes = {
        'detached': ('detach',),
        'abort-on-failure': ('abort',),
        'foreground': ('fg',)
    }
    
    # Normalize mode name
    normalized_mode = None
    for main_mode, aliases in valid_modes.items():
        if mode == main_mode or mode in aliases:
            normalized_mode = main_mode
            break
    
    if not normalized_mode:
        error(f"Invalid COMPOSE_START_MODE: {mode}. Valid values: {', '.join(valid_modes.keys())}")
    
    try:
        if normalized_mode == 'detached':
            info("Starting containers in detached mode")
            run(f"docker compose -f {compose_file} up -d", env=expanded_env)
            
        elif normalized_mode == 'abort-on-failure':
            info("Starting containers in detached mode with health monitoring...")
            run(f"docker compose -f {compose_file} up -d", env=expanded_env)
            
            # Wait for services to become healthy
            success = wait_for_services_health(compose_file, timeout=300, interval=5)
            
            if success:
                info("All services started successfully and are healthy")
                # Post-compose hook execution is handled by main() after start_compose returns
                return
            else:
                error("Service startup failed - not all services became healthy")
                
        elif normalized_mode == 'foreground':
            info("Starting containers in foreground")
            run(f"docker compose -f {compose_file} up", env=expanded_env)
            
    except Exception as e:
        error(f"Failed to start Docker Compose services: {e}")

def main() -> None:
    """
    Main function that orchestrates the infrastructure initialization process.
    
    This function:
    1. Parses command line arguments
    2. Validates required files exist
    3. Generates .env.active from .env.sample (first run only)  
    4. Loads environment variables and performs validation
    5. Resets Docker state if configured
    6. Creates required host directories
    7. Validates Docker Compose configuration
    8. Checks Docker image availability
    9. Starts Docker Compose services
    
    Exits with code 0 on first run after generating .env.active,
    or on successful completion of container startup.
    """
    import argparse
    # Ensure runtime dependencies are present (PyYAML etc.)
    check_imports()
    import yaml
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Python replacement for 'docker compose up' or custom start scripts",
        epilog="""Short help: see --notes for extended usage and notes.""",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
        # The detailed usage/notes block is centralized in usage_notes() below.
        # TODO: KEEP THIS NOTEs BLOCK IN SYNC with any manual docs or README when
        # editing this file. Search for 'KEEP IN SYNC' markers before changing.
    parser.add_argument(
        '-d', '--dir', 
        help="Working directory (default: current directory)", 
        default=os.getcwd(),
        metavar='DIR'
    )
    parser.add_argument(
        '-f', '--file', 
        help="Docker compose file (default: docker-compose.yml)", 
        default="docker-compose.yml",
        metavar='FILE'
    )
    parser.add_argument(
        '-e', '--env', 
        help=f"Environment file name (default: {DEFAULT_ENV_ACTIVE_FILE})", 
        default=DEFAULT_ENV_ACTIVE_FILE,
        metavar='ENV_FILE'
    )
    parser.add_argument('--env-legacy', action='store_true', help='Allow processing legacy .env.sample/.env files (opt-in). By default TOML is required')
    parser.add_argument('--notes', action='store_true', help='Print extended usage notes and exit')
    parser.add_argument('-y', '--yes', action='store_true', help='Assume yes / non-interactive: do not prompt after generating .env')
    parser.add_argument('--external-strict', action='store_true', help='Treat missing EXTERNAL tokens as fatal in non-interactive runs (equivalent to COMPINIT_STRICT_EXTERNAL=1)')
    parser.add_argument('--init-skel-toml', action='store_true', help='Generate a skeleton .env.sample.toml file and exit')
    parser.add_argument('--init-skel-env', action='store_true', help='Generate a skeleton .env.sample file and exit (legacy)')
    parser.add_argument('--migrate-workspace-envs', action='store_true', help='Scan workspace directory and migrate any legacy .env.sample files to TOML (dry-run unless -y provided)')
    
    try:
        args = parser.parse_args()
    except SystemExit:
        # argparse already printed help/error message
        sys.exit(1)

    if getattr(args, 'notes', False):
        print(usage_notes())
        sys.exit(0)
    
    if getattr(args, 'init_skel_toml', False):
        generate_skeleton_toml(DEFAULT_TOML_SAMPLE_FILE)
        sys.exit(0)
    
    if getattr(args, 'init_skel_env', False):
        generate_skeleton_env(DEFAULT_ENV_SAMPLE_FILE)
        sys.exit(0)

    if getattr(args, 'migrate_workspace_envs', False):
        # Walk the working directory and find .env.sample files to migrate
        def _should_ignore_dir(d: str) -> bool:
            ignore = ('.git', '.venv', 'venv', 'node_modules', '__pycache__')
            return any(part in d for part in ignore)

        projects = []
        for root, dirs, files in os.walk(args.dir):
            # skip ignored dirs
            if _should_ignore_dir(root):
                continue
            if '.env.sample' in files or '.env' in files:
                projects.append(root)

        if not projects:
            info("No legacy env files found to migrate in workspace")
            sys.exit(0)

        info(f"Found {len(projects)} projects with legacy env files to migrate")
        for p in projects:
            sample = os.path.join(p, '.env.sample') if os.path.isfile(os.path.join(p, '.env.sample')) else os.path.join(p, '.env')
            out = os.path.join(p, DEFAULT_TOML_SAMPLE_FILE)
            info(f"Would migrate: {sample} -> {out}")
            if getattr(args, 'yes', False):
                try:
                    migrate_env_sample_to_toml(sample, out)
                    info(f"Migrated {sample} -> {out}")
                    try:
                        os.remove(sample)
                        info(f"Removed legacy sample: {sample}")
                    except Exception:
                        warn(f"Failed to remove legacy sample: {sample}")
                except Exception as e:
                    warn(f"Failed to migrate {sample}: {e}")
        if not getattr(args, 'yes', False):
            warn("Dry-run complete. Re-run with -y to perform migration and delete legacy files.")
        sys.exit(0)

    # Propagate CLI flag to environment variable so existing logic can use it
    if getattr(args, 'external_strict', False):
        os.environ['COMPINIT_STRICT_EXTERNAL'] = '1'

    # Global strict mode: when enabled, flip a small set of behaviors to stricter handling.
    # This keeps default behavior permissive for developers but enables stricter checks in CI/production.
    if os.environ.get('COMPINIT_STRICT_MODE', '').lower() in ('1', 'true', 'yes'):
        info("COMPINIT_STRICT_MODE enabled: enabling stricter checks for EXTERNAL tokens, certs, image/registry checks, and network timeouts")
        # Set canonical strict flags only (no legacy fallbacks)
        os.environ.setdefault('COMPINIT_STRICT_EXTERNAL', '1')
        os.environ.setdefault('COMPINIT_STRICT_CERTS', '1')
        # If a registry URL is provided, enable image checking and treat failures strictly.
        if os.environ.get('COMPINIT_MYREGISTRY_URL'):
            os.environ.setdefault('COMPINIT_CHECK_IMAGE_ENABLED', 'true')
            os.environ.setdefault('COMPINIT_STRICT_IMAGE', '1')
        # Treat network/registry timeouts as strict failures
        os.environ.setdefault('COMPINIT_STRICT_NETWORK', '1')
        # Expansion failures should be strict under strict mode
        os.environ.setdefault('COMPINIT_STRICT_EXPANSION', '1')
    
    # Validate and change to working directory
    if not os.path.isdir(args.dir):
        error(f"Working directory '{args.dir}' does not exist")
    
    info(f"PATH is {os.environ.get('PATH', 'NOT_SET')}")
    info(f"Determined script directory: {os.path.dirname(os.path.abspath(__file__))}")
    
    try:
        os.chdir(args.dir)
        info(f"Changing to working directory: {args.dir}")
    except OSError as e:
        error(f"Cannot change to working directory '{args.dir}': {e}")
    
    # Define file paths - TOML-first policy
    toml_file = DEFAULT_TOML_ACTIVE_FILE
    sample_file = DEFAULT_ENV_SAMPLE_FILE
    env_file = args.env
    compose_file = args.file
    
    # Determine configuration format
    use_toml = False
    config_file = None
    # By default require TOML. Legacy .env sample handling is opt-in via --env-legacy.
    if os.path.isfile(toml_file):
        use_toml = True
        config_file = toml_file
        info(f"Using TOML configuration: {toml_file}")
    else:
        if os.path.isfile(sample_file):
            if getattr(args, 'env_legacy', False):
                use_toml = False
                config_file = sample_file
                warn(f"Using legacy .env.sample configuration because --env-legacy was provided: {sample_file}")
            else:
                # Offer migration to TOML
                info(f"Legacy .env.sample detected at {sample_file} but TOML config {toml_file} missing.")
                if getattr(args, 'yes', False) or os.environ.get('COMPINIT_ASSUME_YES', '').lower() in ('1','true','yes'):
                    info("Auto-accepting migration in non-interactive mode")
                    migrate_env_sample_to_toml(sample_file, toml_file)
                    use_toml = True
                    config_file = toml_file
                    # remove legacy sample if migration performed
                    try:
                        os.remove(sample_file)
                        info(f"Removed legacy sample file: {sample_file}")
                    except Exception:
                        warn(f"Failed to remove legacy sample file: {sample_file}")
                else:
                    resp = input(f"Migrate legacy {sample_file} to TOML {toml_file}? [y/N]: ")
                    if resp.strip().lower() in ('y','yes'):
                        migrate_env_sample_to_toml(sample_file, toml_file)
                        use_toml = True
                        config_file = toml_file
                        try:
                            os.remove(sample_file)
                            info(f"Removed legacy sample file: {sample_file}")
                        except Exception:
                            warn(f"Failed to remove legacy sample file: {sample_file}")
                    else:
                        error(f"TOML configuration {toml_file} not found. Re-run with --env-legacy to allow legacy .env.sample usage or create {toml_file}.")
        else:
            error(f"No configuration file found. Please create {toml_file} or provide --env-legacy with {sample_file} present.")
    
    # Validate required files exist
    if not os.path.isfile(compose_file):
        error(f"Intended docker-compose file '{compose_file}' not found. Cannot continue.")
    
    # First run: generate .env from configuration file
    # If a global repo-level TOML exists, merge it with the project TOML and any local per-project overrides
    merged_toml_tmp = None
    merged_toml_tmp = None
    project_config_obj = None
    if use_toml:
        # Create a merged temporary TOML that applies global -> project precedence
        merged_toml_tmp = DEFAULT_TOML_ACTIVE_FILE + '.merged.tmp'
        # search for compose.config.global.toml files up the directory tree is done in _merge_global_project_local_toml
        _merge_global_project_local_toml(DEFAULT_GLOBAL_TOML_FILE, config_file, None, merged_toml_tmp)
        # Parse merged TOML to ProjectConfig so we can use hook declarations and other metadata
        try:
            project_config_obj = parse_toml_config(merged_toml_tmp if os.path.isfile(merged_toml_tmp) else config_file)
        except Exception as e:
            warn(f"Failed to parse merged TOML for project metadata: {e}")


    # Generate runtime environment variables in-memory (TOML-first policy).
    env_vars: Dict[str, str] = {}
    if use_toml:
        # Parse merged TOML to ProjectConfig so we can use hook declarations and other metadata
        cfg_path = merged_toml_tmp if merged_toml_tmp and os.path.isfile(merged_toml_tmp) else config_file
        try:
            config = parse_toml_config(cfg_path)
        except Exception as e:
            error(f"Failed to parse TOML configuration '{cfg_path}': {e}")

        # Validate configuration
        validation_errors = validate_config(config)
        if validation_errors:
            error(f"TOML configuration validation failed:\n  - " + "\n  - ".join(validation_errors))

        info(f"Parsed TOML configuration for project: {config.project_name}")

        # Generate values for variables that need generation (in-memory)
        generated_values: Dict[str, str] = {}
        local_env = dict(os.environ)

        for name, var_config in config.variables.items():
            try:
                if var_config.source == "deferred":
                    generated_values[name] = ""
                    continue

                if var_config.type == "password" and not var_config.value:
                    info(f"Generating password for {name} (type={var_config.type}, length={var_config.length})")
                    generated_values[name] = gen_pw(var_config.length or 32, var_config.charset)
                elif var_config.type == "token" and not var_config.value:
                    if var_config.source == "external":
                        env_fallback = os.environ.get(name)
                        if env_fallback:
                            info(f"Using provided environment value for external token {name}")
                            generated_values[name] = env_fallback
                        else:
                            strict = strict_flag(os.environ, 'EXTERNAL')
                            if os.environ.get('COMPINIT_ASSUME_YES', '').lower() in ('1', 'true', 'yes') and strict:
                                error(f"External token {name} requires input and COMPINIT_STRICT_EXTERNAL=1 enforces strict behavior")
                            if os.environ.get('COMPINIT_ASSUME_YES', '').lower() in ('1', 'true', 'yes'):
                                warn(f"{name} is marked external but no value provided; continuing with empty value")
                                generated_values[name] = ''
                            else:
                                desc = var_config.description or f"External {var_config.type}"
                                warn(f"{name} ({desc}) requires a value. Please enter:")
                                try:
                                    generated_values[name] = input(f"{name}=").strip()
                                except (EOFError, KeyboardInterrupt):
                                    error(f"User input required for {name} but not provided")
                    elif var_config.source == "internal":
                        info(f"Generating internal token for {name} (length={var_config.length})")
                        generated_values[name] = gen_pw(var_config.length or 64, var_config.charset)
                    else:
                        generated_values[name] = var_config.value or ""
                elif var_config.value is not None:
                    try:
                        expanded = expand_vars(var_config.value, local_env)
                        generated_values[name] = expanded
                        local_env[name] = expanded
                    except Exception:
                        generated_values[name] = var_config.value
                        local_env[name] = var_config.value
                else:
                    generated_values[name] = ""
            except Exception as e:
                warn(f"Error generating value for variable {name}: {e}")

        # Convert config to env dict
        env_vars = config_to_env_dict(config, generated_values)

    else:
        # Legacy path: if requested, migrate or parse sample into memory via temporary file
        if getattr(args, 'env_legacy', False):
            import tempfile
            with tempfile.NamedTemporaryFile('w+', delete=False) as tmpf:
                tmp_path = tmpf.name
            try:
                # Parse into a temporary env file and load into memory, then remove temp file
                parse_env_sample(config_file, tmp_path)
                env_vars, _ = load_env_file(tmp_path)
            finally:
                try:
                    os.remove(tmp_path)
                except Exception:
                    pass
        else:
            error(f"TOML configuration {toml_file} not found. Re-run with --env-legacy to allow legacy .env.sample usage or create {toml_file}.")

    # Optionally perform runtime expansion of $VAR and $(cmd) depending on setting
    expansion_enabled = os.environ.get('COMPINIT_ENABLE_ENV_EXPANSION', '').lower() in ('1', 'true', 'yes')
    if expansion_enabled:
        expanded_vars: Dict[str, str] = {}
        for key, value in env_vars.items():
            try:
                expanded_value = expand_vars(value, env_vars)
            except Exception as e:
                warn(f"Error expanding variable {key}: {e}")
                expanded_value = value
            expanded_vars[key] = expanded_value
            if expanded_value != value:
                info(f"Expanded {key} -> {expanded_value}")
        env_vars = expanded_vars
    else:
        info("Runtime expansion disabled: keeping values literal in memory")

    # Update current process environment so subsequent operations see the values
    os.environ.update(env_vars)

    # Process dependencies before starting this project
    dependencies_str = env_vars.get('COMPINIT_DEPENDENCIES', '').strip()
    if dependencies_str:
        step(f"Resolving dependencies: {dependencies_str}")
        try:
            dependency_paths = resolve_dependencies(env_vars, os.getcwd())
            info(f"Dependency resolution order: {' -> '.join([os.path.basename(p) for p in dependency_paths])}")
            start_dependencies(dependency_paths, os.getcwd())
        except Exception as e:
            error(f"Failed to process dependencies: {e}")
        
    # Perform any configured reset actions before creating volumes/directories
        reset_flags = env_vars.get('COMPINIT_RESET_BEFORE_START', 'none')
        if reset_state(env_vars, reset_flags, env_file):
            # env-active was deleted, restart script from the beginning
            python = sys.executable
            os.execv(python, [python] + sys.argv)
        
        # Create required host directories with proper permissions
        step("Creating required directories/volumes with permissions for containers")
        create_hostdirs_from_env(env_vars)
        # Prepare for Docker Compose operations
        step("Starting containers with Docker Compose")

        # TLS certificate files check (canonical variable names only)
        public_key = env_vars.get('PUBLIC_TLS_KEY_PEM')
        public_crt = env_vars.get('PUBLIC_TLS_CRT_PEM')
        if public_key:
            # If cert strict mode is enabled, treat missing/unreadable certs as fatal
            certs_strict = strict_flag(os.environ, 'CERTS')
            if not os.path.isfile(public_key) or not os.access(public_key, os.R_OK):
                if certs_strict:
                    error(f"PUBLIC TLS key file not readable: {public_key}")
                else:
                    warn(f"PUBLIC TLS key file not readable (continuing permissively): {public_key}")
        if public_crt:
            if not os.path.isfile(public_crt) or not os.access(public_crt, os.R_OK):
                if certs_strict:
                    error(f"PUBLIC TLS crt file not readable: {public_crt}")
                else:
                    warn(f"PUBLIC TLS crt file not readable (continuing permissively): {public_crt}")

            # Run pre-compose script if specified (canonical variable name only)
            # Run pre-compose hooks (class-only) and persist returned vars
            pre_meta = run_hooks('PRE_COMPOSE', env_vars, project_config=project_config_obj)
            if pre_meta:
                info(f"Applied {len(pre_meta)} meta changes from pre-compose hooks")
                # Apply values in-memory and process persistence per-variable
                to_env = {}
                project_updates = {}
                local_updates = {}
                env_updates = {}
                sensitive_report = []
                for name, md in pre_meta.items():
                    val = md.get('value')
                    persist = (md.get('persist') or 'auto').lower()
                    sensitive = bool(md.get('sensitive', False))
                    # Write into current process env and env_vars so subsequent steps see it
                    os.environ[name] = '' if val is None else str(val)
                    env_vars[name] = '' if val is None else str(val)
                    to_env[name] = env_vars[name]
                    # Decide persistence target
                    if persist == 'none':
                        # do not persist anywhere
                        if sensitive:
                            sensitive_report.append(name)
                        continue
                    if persist == 'env':
                        env_updates[name] = env_vars[name]
                        continue
                    if persist == 'local':
                        local_updates[name] = env_vars[name]
                        continue
                    if persist == 'project':
                        project_updates[name] = env_vars[name]
                        continue
                    # auto: infer from sensitivity and default policy
                    if persist == 'auto' or persist == '':
                        if sensitive:
                            # sensitive defaults to NOT persisted unless operator opts-in
                            sensitive_report.append(name)
                        else:
                            # non-sensitive auto -> project if available else local
                            # New policy: always consider project persistence for non-sensitive auto returns
                            if os.path.isfile(config_file):
                                project_updates[name] = env_vars[name]
                            else:
                                local_updates[name] = env_vars[name]

                # Persist according to accumulated buckets  new policy: persist into active TOML only; do not write a .env file
                try:
                    # Merge project_updates and local_updates (and any sensitive) into the active project TOML
                    merged_project_updates = {}
                    merged_project_updates.update(project_updates)
                    merged_project_updates.update(local_updates)
                    if sensitive_report:
                        merged_project_updates.update({k: env_vars[k] for k in sensitive_report})

                    if merged_project_updates:
                        # Write into the active TOML (project-level file)
                        target_toml = config_file if os.path.isfile(config_file) else DEFAULT_TOML_ACTIVE_FILE
                        _write_local_toml_variables(target_toml, merged_project_updates)

                    # Env-only updates (env_updates and to_env) are applied in-memory and passed to hooks/compose; do not persist to disk
                    if env_updates or to_env:
                        info(f"Applied {len(env_updates) + len(to_env)} runtime-only hook variables in-memory (not persisted to a .env file)")

                    if sensitive_report:
                        info(f"Sensitive hook variables processed and persisted to active TOML: {', '.join(sensitive_report)}")
                except Exception as e:
                    warn(f"Unable to persist pre-compose hook variables: {e}")
        compose_mode = env_vars.get('COMPINIT_COMPOSE_START_MODE', 'abort-on-failure')
        info(f"COMPINIT_COMPOSE_START_MODE set to: {compose_mode}")
        
        # Validate compose config and check env var usage
        config_yaml = check_compose_yaml(compose_file, env_vars, env_file)
        
        # Check image availability before attempting to start (toggleable)
        check_images_enabled = env_vars.get('COMPINIT_IMAGE_CHECK_ENABLED', 'false').lower() in ('1', 'true', 'yes')
        if not check_images_enabled:
            info("Image availability check is disabled (COMPINIT_IMAGE_CHECK_ENABLED=false). Skipping image checks.")
            images = []
        else:
            # Optionally verify COMPINIT_MYREGISTRY_URL reachability before checking images
            myreg = env_vars.get('COMPINIT_MYREGISTRY_URL')
            ok, msg = check_myregistry(myreg, env_vars)
            if not ok:
                warn(f"COMPINIT_MYREGISTRY_URL check failed: {msg}")
            else:
                info(f"COMPINIT_MYREGISTRY_URL check: {msg}")
            step(f"Checking image availability for all referenced images in {compose_file}...")
            images = get_images_from_compose(compose_file, env_vars)
            if images:
                info(f"Found {len(images)} images to check: {', '.join(images)}")
                available, missing = check_images_parallel(images)
                check_for_image_updates(images)
                if missing:
                    error(f"Some images are missing or inaccessible: {', '.join(missing)}\n"
                          f"Please ensure Docker is running and images are available.",
                          show_stacktrace=False)
                info(f"All {len(available)} required images are available")
            else:
                warn("No images found in docker-compose configuration")
        
    # Start Docker Compose services, passing runtime env_vars (in-memory)
    start_compose(compose_file, compose_mode, env_vars, env_file)

    # Run post-compose hooks (class-only) and persist returned vars
    post_meta = run_hooks('POST_COMPOSE', env_vars, project_config=project_config_obj)
    if post_meta:
        info(f"Applied {len(post_meta)} meta changes from post-compose hooks")
        # Apply same per-variable persistence model as pre-hooks
        to_env = {}
        project_updates = {}
        local_updates = {}
        env_updates = {}
        sensitive_report = []
        for name, md in post_meta.items():
            val = md.get('value')
            persist = (md.get('persist') or 'auto').lower()
            sensitive = bool(md.get('sensitive', False))
            os.environ[name] = '' if val is None else str(val)
            env_vars[name] = '' if val is None else str(val)
            to_env[name] = env_vars[name]
            if persist == 'none':
                if sensitive:
                    sensitive_report.append(name)
                continue
            if persist == 'env':
                env_updates[name] = env_vars[name]
                continue
            if persist == 'local':
                local_updates[name] = env_vars[name]
                continue
            if persist == 'project':
                project_updates[name] = env_vars[name]
                continue
            # auto
            if sensitive:
                sensitive_report.append(name)
            else:
                # New policy: always persist auto non-sensitive returns into project active TOML when project TOML exists
                if os.path.isfile(config_file):
                    project_updates[name] = env_vars[name]
                else:
                    local_updates[name] = env_vars[name]

        try:
            # Merge project/local updates and sensitive values and persist to active TOML
            merged_project_updates = {}
            merged_project_updates.update(project_updates)
            merged_project_updates.update(local_updates)
            if sensitive_report:
                merged_project_updates.update({k: env_vars[k] for k in sensitive_report})
            if merged_project_updates:
                target_toml = config_file if os.path.isfile(config_file) else DEFAULT_TOML_ACTIVE_FILE
                _write_local_toml_variables(target_toml, merged_project_updates)

            # Env-only updates remain in-memory and are not written to disk
            if env_updates or to_env:
                info(f"Applied {len(env_updates) + len(to_env)} runtime-only post-hook variables in-memory (not persisted to a .env file)")

            if sensitive_report:
                info(f"Sensitive hook variables processed and persisted to active TOML: {', '.join(sensitive_report)}")
        except Exception as e:
            warn(f"Unable to persist post-compose hook variables: {e}")

    info("Infrastructure initialization completed successfully!")
    return

if __name__ == "__main__":
    main()